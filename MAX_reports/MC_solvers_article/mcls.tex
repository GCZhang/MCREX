\section{Stochastic Linear Solvers}

We consider the solution of systems of linear equations of the form
\begin{equation}
A \mathbf{x}=\mathbf{b},
\label{linsys}
\end{equation}
where $A\in \mathbb{R}^{n\times n}$, $\mathbf{x}$, $\mathbf{b} \in
\mathbb{R}^n$.

By applying a left preconditioning, the system (\ref{linsys}) gets the
following shape:
\begin{equation}
P^{-1}A \mathbf{x}=P^{-1}\mathbf{b},
\label{linsys_prec}
\end{equation}
(\ref{linsys_prec}) can be
reinterpreted as a fixed point scheme
\begin{equation}
 \mathbf{x}=H\mathbf{x}+\mathbf{f}.
 \label{fixedpoint}
\end{equation}
where $H=I-P^{-1}A$ and $\mathbf{f}=P^{-1}\mathbf{b}$.

Assuming that the spectral radius $\rho(H)<1$, the solution to
(\ref{fixedpoint}) can be written in terms of a power series of
$H$ (Neumann series)
\[
\mathbf{x}=\sum_{i=0}^\infty H^i\mathbf{f}.
\]
Therefore the fixed point scheme generates a sequence of approximate solutions
$\{\mathbf{x}^{(k)}\}_{k=0}^{\infty}$ which converges to the exact solution
regardless of the initial guess $\mathbf{x}_0$.

By restricting the attention to a single component of $\mathbf{x}$ we
have
\begin{equation}
x_i=\sum_{\ell=0}^\infty \sum_{k_1}^n\sum_{k_2}^n\cdots \sum_{k_{\ell}=1}^n
H_{k_0,k_1}H_{k_1,k_2}\cdots H_{k_{\ell-1}, k_{\ell}}f_{k_{\ell}}.
\label{forward}
\end{equation}
The last equation can be reinterpreted as the realization of an estimator
defined on a random walk.

Let us start considering a random walk whose
state space $S$ is characterized by the set of indices of the forcing term
$\mathbf{f}$:
\[
S=\{1,2,\cdots, n\} \subset \mathbb{N}.
\]
Each $i$-th step of the random walk has a random variable
$k_i$ associated with it. The realization of $k_i$ represents the index of the
component of $\mathbf{f}$
which is visited in the current step of the random walk.

The way the transition probability is built and the way the initial state of
the random walk is chosen give birth to two different approaches we are
going to use: the \textit{Forward} and \textit{Adjoint} methods.

\subsection{Forward Method}

The goal is to evaluate a functional such as
\[
J(\mathbf{x})=(\mathbf{h},\mathbf{x})=\sum_{i=1}^n h_i x_i.
\]
where $\mathbf{h}\in \mathbb{R}^n$ is the Riesz representative in
$\mathbb{R}^n$ of the functional $J$.
We can use it to build the initial probability $\tilde{p}:
S\rightarrow [0,1]$ of the random walk such that
\[
\tilde{p}(k_0=i)=\tilde{p}_{k_0}=\frac{\lvert h_i\rvert}{\sum_{i=1}^n \lvert
h_i\rvert}.
\]
It is important to stress out that the role of vector $\mathbf{h}$ is
restricted to the construction of the initial probability. Once that is done,
it is
not used anymore in the definition of the stochastic process.
A possible choice for the transition probability $P$ can be
\[
p(k_i=j \;\lvert\;k_{i-1}=i )=P_{i,j}=\frac{\lvert H_{i,j}\rvert}{\sum_{k=1}^n
\lvert H_{i,k}\rvert}.
\]
where $\tilde{p}(\cdot,i):S\rightarrow [0,1]$ $\forall i\in S$.
A related sequence of random variables $w_{i,j}$ can be defined
such that
\[
w_{i,j}=\frac{H_{i,j}}{P_{i,j}}.
\]
The probability distribution of the random variables $w_{i,j}$ is represented
by the transition matrix that rules the stochastic process. The $w_{i,j}$'s
just introduced can be used to build one more sequence
of random variables.
At first we introduce quantities $W_j$
\[
W_{0}=\frac{h_{k_0}}{\tilde{p}_{k_0}}, \quad W_j=W_{j-1} w_{i,j}, \quad
j=1,\cdots, i.
\]
By defining
\[
X(\nu)=\sum_{m=0}^k W_m f_{i_m}
\]
as the random variable associated with a specific permutation $\nu$, we can
define the estimator $\theta_i$ such as
\[
\theta=E[X]=\sum_{\nu}P_{\nu}X(\nu).
\]
The integer $n$ represents the size of the solution vector to (\ref{linsys})
and
the
index
$i$
is referred to the component of the solution vector we want to compute.
$P_{\nu}$ is the probability associated with a specific permutation of the
random walk.
It can be proved that
\[
E[W_i f_{k_i}]=(\mathbf{h},H^i\mathbf{f}), \quad i=0,1,2,\cdots
\]
and
\[
\theta_i=E\bigg[\sum_{i=0}^\infty W_i f_{k_i}\bigg]=(\mathbf{h},\mathbf{x}).
\]

A possible choice for $\mathbf{h}$ is a vector of the standard basis. This
would correspond in setting manually the initial state of the random walk,
which turns the related initial probability into a Kronecker delta
\[
\tilde{p}(k_0=i)=\delta_{i,j}.
\]
By doing so, we have
\begin{equation}
\theta_i=E\bigg[\sum_{\ell=0}^\infty W_{\ell}
f_{k_{\ell}}\bigg]=x_i=\sum_{l=0}^\infty
\sum_{k_1=1}^{n}\sum_{k_2=1}^n\cdots \sum_{k_{\ell}=1}^n
P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},
k_{\ell}}w_{k_0,k_1}w_{k_1,k_2}\cdots
w_{k_{\ell-1}, k_{\ell}}f_{k_{\ell}}.
\label{dir_mean}
\end{equation}

As regards the variance, we remember that the following relation holds:
\begin{equation}
Var\bigg [\sum_{\ell=0}^\infty W_{\ell}
f_{k_{\ell}}\bigg]=E\bigg[\sum_{\ell=0}^\infty W_{\ell}^2
f_{k_{\ell}}^2\bigg] - \bigg (E\bigg[\sum_{\ell=0}^\infty W_{\ell}
f_{k_{\ell}}\bigg]\bigg )^2.
\label{dir_var}
\end{equation}

Hence the variance can be computed as the difference between the second
moment of the random variable and the square of the first moment.\newline

In order to apply the Central Limit Theorem (CLT) to the estimators defined
above, we must require that
the estimators have both finite expected value and variance. This is
equivalent in
checking the finiteness of the expected value and of the second moment.
Therefore we have to impose the following conditions:

\begin{equation}
 E\bigg[\sum_{\ell=0}^\infty W_{\ell} f_{k_{\ell}}\bigg]<\infty
\end{equation}

\begin{equation}
 E\bigg[\sum_{\ell=0}^\infty W_{\ell}^2
f_{k_{\ell}}^2\bigg]<\infty
\end{equation}

The Forward method presented above, however, has the limitation of employing an
entire set of permutations to estimate just a single entry of
the solution at a time. Therefore, in order to estimate the entire solution
vector for \ref{linsys_prec}, we have to employ as many sets of permutation as
the number of entries composing the vector. Of course this might turn to be
extremely expensive both in terms of time and resources for most of the
challenging problems coming from applications. This is essentially the reason
why usually it is preferred to adopt another stochastic algorithm which is
explained here following: the \textit{Adjoint} Monte Carlo method.

\subsection{Adjoint Method}

A second Monte Carlo method can be derived by considering the linear system
adjoint to (\ref{linsys_prec})
\begin{equation}
(P^{-1}A)^T\mathbf{y}=\mathbf{d},
\label{adj}
\end{equation}
where $\mathbf{y}$ and $\mathbf{d}$ are the adjoint solution and source term.

By reformulating the fixed point scheme, introducing initial probability,
transition probability and weights
sequences in the same fashion as done before, the expected value for the
estimator becomes
\begin{equation}
\theta_j=E\bigg[\sum_{\ell=0}^\infty W_{\ell}\delta_{k_{\ell},
j}\bigg]=\sum_{\ell=0}^{\infty}\sum_{k_1}^n\sum_{k_2}^n\cdots\sum_{k_{\ell}}^n
f_{k_0}P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},K_{\ell}}w_{k_0,k_1}\cdots
w_{k_{\ell-1},k_{\ell}}\delta_{k_{\ell},j}.
\label{adj_mean}
\end{equation}
This estimator is known in literature as \textit{collision} estimator.

We can notice now that the component of the source vector, in each
permutation of the random walk, is associated with the initial step. The
Kronecker delta
at the end of the series stands for a filter. It means that if the goal is to
estimate the $j$-th component of the solution vector, just a subset of all the
permutations gives a contribution to this. The only permutations considered are
those ones that currently reside on the index associated with the entry to be
estimated.

The variance is

\begin{equation}
Var\bigg [\sum_{\ell=0}^\infty W_{\ell}
f_{k_0}\delta_{k_{\ell},j}\bigg]=E\bigg[\sum_{\ell=0}^\infty W_{\ell}^2
f_{k_0}^2\delta_{k_{\ell},j}\bigg ] - \bigg (E\bigg[\sum_{\ell=0}^\infty
W_{\ell}
f_{k_0}\delta_{k_{\ell},j}\bigg]\bigg )^2\quad j=1,\cdots,n
\label{adj_var}.
\end{equation}

On the same lead of what done for the Forward method, there is need to impose
finiteness of the expected value and second moment. Therefore the following
conditions must be verified:

\begin{equation}
 E\bigg[\sum_{\ell=0}^\infty W_{\ell}\delta_{k_{\ell},
j}\bigg]<\infty \quad j=1,\cdots,n
\end{equation}
and
\begin{equation}
 E\bigg[\sum_{\ell=0}^\infty W_{\ell}^2
f_{k_0}^2\delta_{k_{\ell},j}\bigg]<\infty \quad j=1,\cdots,n.
\end{equation}

The main advantage of this method, compared to the Forward one, consists in the
fact that a single set of permutation is used to estimate the entire solution
vector. Therefore, in general, this algorithm has to be preferred to the
previous one since it enables to reduce dramatically the computational burden.

In literature another estimator is employed along with the Adjoint Monte Carlo
method, the so called \textit{expected value} estimator. Its
formulations is as follows:

\begin{equation}
\theta_j=E\bigg[f_j + \sum_{\ell=0}^\infty
W_{\ell}H_{k_{\ell}, j}^T\bigg]=f_j
+ \sum_{\ell=0}^{\infty}\sum_{k_1}^n\sum_{k_2} ^n\cdots\sum_ { k_ { \ell}}^n
f_{k_0}P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},K_{\ell}}w_{k_0,k_1}\cdots
w_{k_{\ell-1},k_{\ell}}H_{k_{\ell},j}^T.
\label{adj_mean1}
\end{equation}

Differently from the collision estimator, the expected value estimator averages
the deterministic contribution of the iteration matrix over all the potential
states $j$ that might be reached from the current state $\ell$. The variance
in this case becomes:

\begin{equation}
Var\bigg [\sum_{\ell=0}^\infty W_{\ell}
f_{k_0}H_{k_{\ell},j}^T\bigg]=E\bigg[\sum_{\ell=0}^\infty W_{\ell}^2
f_{k_0}^2 {H_{k_{\ell},j}^T}^T\bigg ] - \bigg (E\bigg[\sum_{\ell=0}^\infty
W_{\ell}
f_{k_0}H_{k_{\ell},j}^T\bigg]\bigg )^2\quad j=1,\cdots,n
\label{adj_var}.
\end{equation}

In the sections dedicated to the numerical tests, both the collision estimator
and the expected value are employed, in order to test their effectiveness.

\subsection{Hybrid Stochastic/Deterministic Methods}

Even if the standard Monte Carlo, in both its Forward and Adjoint formulation,
has the advantage of being embarrassingly parallelizable (because of the
independence of the samplings employed), it has a drawback as
well. In fact its slow rate of convergence, associated with the
$\frac{1}{\sqrt{N}}$ behavior predicted but the CLT, prevents its use for
problems of actual interest in applied linear algebra (with many d.o.f's).

Moreover, when the spectral radius of the iteration matrix is close to the
unit
value, Monte Carlo direct methods reach convergence after an excessive number
of steps for each
permutation. In particular this aspect will be pointed out more by some
numerical tests analyzed in the upcoming sections of this work.
This motivation makes the standard Mote Carlo paradigm even more
unpractical for solving linear systems associated with realistic applications.
\newline

To cope with this, in the early 2000's Evans et al. introduced a new method:
the
\textit{Monte Carlo Synthetic Acceleration (MCSA)}
(\cite{ESW2013} and \cite{EMSH2014}). Earlier contributions on these direction
are attributed also to other scientists (e.g. \textit{Sequential Method} by
J.H. Halton). However the MCSA is the algorithm we are going to focus on more
hereafter. If
we think about a fixed point formulation of the problem
such as $\mathbf{x}=H\mathbf{x}+\mathbf{b}$, then the Monte Carlo Synthetic
Acceleration assumes this form

\begin{algorithm}[H]
 \KwData{$H$, $\mathbf{b}$, $\mathbf{x}_0$}
 \KwResult{$x_{num}$}
 $\mathbf{x}^{l}=\mathbf{x}_0$\;
 \While{not reached convergence}{
  $\mathbf{x}^{l+\frac{1}{2}}=H\mathbf{x}^l+\mathbf{b}$\;
  $\mathbf{r}^{l+\frac{1}{2}}=\mathbf{b}-A\mathbf{x}^{l+\frac{1}{2}}$\;
  $\delta \mathbf{x}^{l+\frac{1}{2}}=(I-H)^{-1}\mathbf{r}^{l+\frac{1}{2}}$\;
  $\mathbf{x}^{l+1}=\mathbf{x}^{l+\frac{1}{2}}+\delta
\mathbf{x}^{l+\frac{1}{2}}$\;
 }
 $x_{num}=x^{l+1}$\;
 \caption{Monte Carlo Synthetic Acceleration}
\end{algorithm}
The Monte Carlo method is used to compute the updating contribution $\delta
\mathbf{x}^{l+\frac{1}{2}}$.

The role of the first line of the iterative process has the goal to compress
the residual of the previous iteration. This is the actual difference
between the MCSA and the Sequential Monte Carlo. Once this done, the corrected
residual
is employed to compute an update $\delta x^{1+\frac{1}{2}}$ to the solution via
one of the Monte Carlo techniques introduced before. The utility of compressing
the residual at the very beginning of the iteration will be shown in the two
last test cases for numerical validations.
The intent of MCSA is to designate most of the time employed by the
computation to the accomplishment of the stochastic part of the algorithm.
This
is useful to decrease the probability of fault occurrence in the non-resilient
steps, which is represented by the non-stochastic component of the numerical
scheme presented above. In particular, the higher is the percentage of time
spent on the computation of $\delta x^{1+\frac{1}{2}}$ via Monte Carlo, the
better it should be in terms of resilience.

\subsubsection{Sequential Monte Carlo}

\subsubsection{Monte Carlo Synthetic Acceleration}

