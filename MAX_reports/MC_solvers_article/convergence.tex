\section{Convergence Behavior of Stochastic Methods}
\label{sec:convergence}

The convergence requirements imposed by the Monte Carlo estimator and
the corresponding variance can be reformulated in a purely deterministic
setting by considering certain properly defined matrices.
For instance, the condition of finiteness of the expected value can be
reformulated by requiring
\begin{equation}
 \rho(H)<1
 \label{rho_h},
\end{equation}
where $H$ is the iteration matrix of the fixed point scheme.
In fact both equations (\ref{dir_mean}) and (\ref{adj_mean}) can be considered
as
power series in terms of $H$ and the spectral radius of $H$ smaller
than 1 is a necessary and sufficient condition for the Neumann series to
converge.

We wish to achieve a similar result based on the requirement that the
second moment of the estimator must be finite as well.
Examination of the equations of the variance (\ref{dir_var}) and (\ref{adj_var})
for
the forward and the adjoint method respectively, we notice that the second
moment can be reinterpreted as a power series with respect to the matrices
defined as follows:
\[
 \hat{H}_{i,j}=\frac{H_{i,j}^2}{P_{i,j}} \quad \text{- Forward Method}.
\]
or
\[
 \hat{H}_{i,j}=\frac{H_{j,i}^2}{P_{i,j}} \quad \text{- Adjoint Method}.
\]
Accordingly to the necessary and sufficient condition for the power series to
converge, we require
\begin{equation}
 \rho(\hat{H})<1.
 \label{rho_star}
\end{equation}
Condition (\ref{rho_h}) must be required for a generic fixed point scheme to
reach convergence, whereas the extra condition (\ref{rho_star}) is typical of
the stochastic schemes presented in this work.

\subsection{Necessary and Sufficient Conditions}

Here we briefly reproduce some results introduced in \cite{MASC2013}
concerning necessary conditions and sufficient conditions for convergence.
In particular, a suitable choice for constructing a
transition probability is discussed to guarantee convergence.

The construction of the transition probability must satisfy
\[
\begin{cases}
  P_{i,j}\ge 0 \\
 \sum_{j=1}^N P_{i,j}\le 1 \,.
\end{cases}
\]
One additional constraint relates the sparsity pattern of $H$ to that
of the transition probabilities:
\begin{align*}
& \text{Forward Method:} \quad H_{i,j}\ne 0 \Rightarrow P_{i,j}\ne 0 \\ &
\text{Adjoint Method:} \quad H_{j,i}\ne 0 \Rightarrow P_{i,j}\ne 0
\end{align*}.
The previous constraints on the values for the entries of the
transition probability are called \textit{transition conditions}.

An introductory theoretical result is provided to motivate the possible
choices for the transition probability.

\begin{thm}
 Consider a vector $a=(a_1,a_2,\cdots,a_N)^T$ where at least one element is
nonzero, $a_k\ne0$ for some $k\in\{1,\cdots,N\}$.
\begin{itemize}
 \item For a probability vector $p=(p_1,p_2,\cdots,p_N)^T$ satisfying the
transition conditions, the lower bound of $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}$ is $\bigg(\sum_{k=1}^N \lvert a_k\rvert\bigg)^2$
\item There always exists a probability vector such that $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}\ge c\ge 1$ for all $c>1$.
\end{itemize}
\label{lemma}
\end{thm}

It might be observed that the minimum of the quantity $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}$ is attained when the probability vector is
defined such as $\displaystyle p_k=\frac{\lvert a_k\rvert}{\sum_{k=1}^N \lvert
a_k\rvert}$. It implies that the $\infty$-norm of $\hat{H}$ is minimized as
much
as possible.

For the sake of simplicity let us introduce now a generic random walk
truncated at a certain $k$-th step
\[
 \gamma_k:\; r_0\rightarrow r_1 \rightarrow r_2 \rightarrow \cdots \rightarrow
r_k
\]
and the statistical estimator
\[
 X(\gamma_k)=\frac{H_{r_0,r_1}H_{r_1,r_2}\cdots
H_{r_{k-1},r_k}}{P_{r_0,r_1}P_{r_1,r_2}\cdots P_{r_{k-1},r_k}}f_{r_k}
\]
which is associated with the forward method.

Then the following theorem holds
\begin{thm}\textit{(Suited to the Forward Method)}

Let $H\in \mathbb{R}^{n\times n}$ such that $\lVert H\rVert_{\infty}<1$.
Consider $\nu_k$ as the realization of a random walk $\gamma$ truncated at the
$k$-th step. Then,
there always exists a
transition matrix $P$ such that
$Var\Big(X(\nu_k)\Big)\rightarrow 0$ and
$Var\Big(\sum_{\nu}X(\nu_k)\Big)$ is bounded as $k\rightarrow \infty$.
\label{for_thm}
\end{thm}

If we introduce the estimator
\[
 X(\gamma_k)=\frac{H^T_{r_0,r_1}H^T_{r_1,r_2}\cdots
H^T_{r_{k-1},r_k}}{P_{r_0,r_1}P_{r_1,r_2}\cdots
P_{r_{k-1},r_k}}sign(f_{r_0})\lVert \mathbf{f}\rVert_1
\]
which is associated with the adjoint method, then we can state a
theorem specular to \ref{for_thm}.

\begin{thm}\textit{(Suited to the Adjoint Method)}

 Let $H\in \mathbb{R}^{n\times n}$ with $\lVert H\rVert_{1}<1$.
Consider $\nu_k$ as the realization of a random walk $\gamma$ truncated at the
$k$-th step. Then,
there always exists a
transition matrix $P$ such that
$Var\Big(X(\nu_k)\Big)\rightarrow 0$ and
$Var\Big(\sum_{\nu}X(\nu_k)\Big)$ is bounded as $k\rightarrow \infty$.
\label{adj_thm}
\end{thm}

These theoretical results represent sufficient (but not necessary) conditions
for the convergence
of the forward and adjoint Monte Carlo and can be easily verified.
However, in many cases the conditions $\lVert H\rVert_{\infty}<1$ or
$\lVert H\rVert_1<1$ may be restrictive.

\subsection{Construction of Transition Probabilities}

The method for determining the transition probability has a significant impact
on the properties of the resulting approach, and in many circumstances the
choice can be the difference between convergence or divergence of the stochastic
scheme.  Two primary approaches have been considered in the literature:
\textit{uniform probabilities} and \textit{weighted probabilities}.

\subsubsection{Uniform probabilities}

With this approach, the transition matrix $P$ is such that all the non-zero elements
in each row have equal probability of occurring:
\[
\text{Forward}:\;P_{i,j}=
\begin{cases}
0 \quad \quad \quad \qquad \qquad \qquad \text{if}\quad H_{i,j}=0 \\
\frac{1}{\#(\text{non-zeros in the row)}} \quad \text{if} \quad H_{i,j}\ne 0
\end{cases}\quad
\]
\[
\text{Adjoint}:\;P_{i,j}=
\begin{cases}
0 \quad \quad \quad \qquad \qquad \qquad \text{if}\quad H_{j,i}=0 \\
\frac{1}{\#(\text{non-zeros in the column)}} \quad \text{if} \quad H_{j,i}\ne 0
\end{cases}
\]
The Monte Carlo approach resorting to this definition of the transition matrix,
in accordance to \cite{AADBTW2005}, is called \textit{Uniform Monte Carlo} (UM).

\subsubsection{Weighted probabilities}

An alternative way of defining the transition matrix consists in attributing
probabilities to the entries of the transition probability according
to the magnitude of the elements. For instance we may employ the following
definition
\[
\text{Forward}: \;
p(k_i=j \;\lvert\;k_{i-1}=i )=P_{i,j}=\frac{\lvert
H_{i,j}\rvert^p}{\sum_{k=1}^n
\lvert H_{i,k}\rvert^p}
\]
and
\[
\text{Adjoint}: \;
p(k_i=j \;\lvert\;k_{i-1}=i )=P_{i,j}=\frac{\lvert
H_{j,i}\rvert^p}{\sum_{k=1}^n
\lvert H_{k,i}\rvert^p} \,,
\]
where $p\in \mathbb{N}$.
This selection, for $p=1$, has been termed
\textit{Monte Carlo Almost Optimal} (MAO).
The reason for the ``almost optimal'' designation can been seem from
Theorem~\ref{lemma}, as the quantity
$\displaystyle \sum_{k=0}^N\frac{a_k^2}{p_k}$
is minimized when the probability vector is
defined such as $\displaystyle p_k=\frac{\lvert a_k\rvert}{\sum_{k=1}^N \lvert
a_k\rvert}$. This implies that the almost optimal probability minimizes the
$\infty$-norm of $\hat{H}$ in the forward method and the $1$-norm of $\hat{H}$
in the adjoint method. This is very important, since these two norms
provide computable upper bounds for $\rho(\hat{H})$.
In Section~\ref{sec:results}, it will be shown
that the almost optimal probability enables convergence of the MC solver,
even in cases when the employment of the uniform probability does not provide
convergence at all.

\subsection{Classes of Matrices with Guaranteed Convergence}

As we have already seen, sufficient conditions for
the convergence of the Monte Carlo linear solvers are very restrictive (see
\cite{MASC2013}), whereas the necessary and sufficient condition requires the
computation of $\rho(H)$and $\rho(\hat{H})$, which is very expensive in general.
Many instances of matrices (perhaps with appropriate preconditioning) such
that $\rho(H)<1$ are available, but verifying $\rho(\hat{H})<1$ is generally
more problematic.  We now with to identify classes of matrices for which
the Monte Carlo linear solver is guaranteed to converge.

\subsubsection{Strictly Diagonally Dominant}
\label{sec:sdd}

One of the these sets is represented by strictly diagonally dominant matrices.

\begin{defn}
 A matrix $A\in\mathbb{R}^{n\times n}$is strictly diagonally dominant (s.d.d)
by rows if
 \begin{equation}
    \lvert a_{ij}\rvert>\sum_{\substack{i=1\\ i\ne j}}^{i=n}\lvert
a_{ij}\rvert
\label{sddr}
 \end{equation}
\end{defn}

\begin{defn}
 A matrix $A\in\mathbb{R}^{n\times n}$is strictly diagonally dominant (s.d.d)
by columns if
 \begin{equation}
    \lvert a_{ij}\rvert>\sum_{\substack{j=1\\ j\ne i}}^{j=n}\lvert
a_{ij}\rvert
\label{sddc}
 \end{equation}
\end{defn}

When (\ref{sddr}) holds we can resort to a left diagonal preconditioning and we
get an iteration matrix $H=I-D^{-1}A$ such that $\lVert H \rVert_{\infty}<1$.

Introducing a MAO transition probability for the forward method we get
\[
 P_{ij}=\frac{\lvert H_{ij}\rvert}{\sum_{k=1}^n\lvert H_{ik}\rvert}
\]
therefore the entries of $\hat{H}$ are defined as follows
\[
 \hat{H}_{ij}=\frac{H^2_{ij}}{P_{ij}}=\lvert H_{ij}\rvert\bigg
(\sum_{k=1}^n\lvert
H_{ik}\rvert\bigg ).
\]
Therefore
\[
 \sum_{j=1}^n\lvert \hat{H}_{ij} \rvert = \sum_{j=1}^n \hat{H}_{ij} =\bigg
(\sum_{j=1}^n \lvert H_{ij}\rvert\bigg )\bigg (\sum_{k=1}^n\lvert
H_{ik}\rvert\bigg ) =\bigg
(\sum_{j=1}^n \lvert H_{ij}\rvert\bigg ) ^2 <1 \quad \forall i=1,\cdots, n.
\]
This automatically implies that $\rho(\hat{H})<=\lVert
\hat{H}\rVert_{\infty}<1$. Thus the forward Monte Carlo converges.
However nothing a priori can be
stated about the behavior of the adjoint method.

Instead if (\ref{sddc}) holds we can resort to a right diagonal preconditioning
and we
get an iteration matrix $H=I-AD^{-1}$ such that $\lVert H \rVert_{1}<1$.
In this case, by following a similar reasoning to the one made before, we
conclude that the adjoint method convergences thanks to $\lVert
\hat{H}\rVert_1<1$,
whereas nothing can be said in advance as concerns the forward method.

\subsubsection{Generalized Diagonally Dominant}
\label{sec:gdd}

 Another set of matrices for which the convergence of
the MC solvers is guaranteed is made of \textit{generalized diagonally
dominant matrices}.

\begin{defn}
A square matrix $A\in\mathbb{R}^{n\times n}$ is said to be generalized
diagonally dominant if
\[
 \lvert a_{ii}\rvert x_i \ge \sum_{\substack{j=1\\j\ne i}}^n \lvert
a_{ij}\rvert
x_j, \quad i=1,\cdots,n
\]
for some positive vector $\mathbf{x}=(x_1,\cdots,x_n)^T$.
\end{defn}

A proper subset of the generalized diagonally dominant matrices is represented
by $M$- matrices (see \cite{Ax1996}).
There are many definitions of $M$-matrices that may be found on books and all
of them are equivalent. The one presented in \cite{Saad} states the
following
\begin{defn}
A matrix $A\in\mathbb{R}^{n\times n}$ is said to be an $M$-matrix if it
satisfies the following four properties:
\begin{itemize}
 \item $a_{i,i}>0$ for $i=1,\cdots,n$
 \item $a_{i,j}\le 0$ for $i\ne j$, $i,j=1,\cdots,n$
 \item $A$ is nonsingular
 \item $A^{-1}\ge 0$
\end{itemize}
\end{defn}


A very important result about $M$-matrices enables to come up with a convergent
splitting and, therefore, a convergent fixed point scheme.
In particular Jacobi, Block Jacobi and Gauss Seidel  are
convergent
splitting for an $M$-matrix.
However, the construction of a splitting like this is not enough to ensure the
convergence of
the Monte Carlo linear solver. In fact none of these conditions is sufficient
to guarantee $\rho(\hat{H})<1$. Because of this obstacle, other options must
be considered.

For this purpose we
appeal to a result in \cite{Li2002}. In this paper the author presents an
algorithm to transform a generalized diagonally dominant matrix with
nonzero diagonal entries into strictly diagonally dominant. The
algorithm works for a generic complex matrix $A\in\mathbb{C}^{n\times n}$.

Here below we report the algorithm at hand

\begin{algorithm}[H]
 For a given complex matrix $A$, $a_{ii}\ne 0$, $i=1,\cdots, n$;\newline
 \begin{enumerate}
  \item Compute $S_{i}=\sum_{\substack{j=1 \\ j\ne i}^{n}}\lvert
a_{ij}\rvert$,
$i=1,2,\cdots,n$
\item Set $t=0$. For $i=1,2,\cdots, n$, if $\lvert a_{ii}\rvert>S_i$, then set
$t=t+1$
\item If $t=0$, then print "A is not a GDDM": END
\item If $t=n$, then print "A is a GDDM": END
\item \For{i=1,n}{$d_{i}=\frac{S_i+\varepsilon}{\lvert
a_{ii}\rvert+\varepsilon} \quad
\varepsilon>0,\quad j=1,2,\cdots,n$\;
$a_{ji}=a_{ji}\cdot d_i$}
\item Go to step 1.
 \end{enumerate}
 \caption{Algorithm to turn a GDDM matrix into a s.d.d by rows.}
\end{algorithm}

This approach turns a generalized diagonally dominant matrix into a
strictly diagonally dominant matrix by rows. By substituting
$\displaystyle S_{i}=\sum^{n}_{\substack{j=1 \\ j\ne i}}\lvert a_{ij}\rvert$
at step 1 with
$\displaystyle S_{i}=\sum^{n}_{\substack{i=1 \\ i\ne j}}\lvert a_{ij}\rvert$
and by replacing
$a_{ji}=a_{ji}\cdot d_i$ with $a_{ji}=a_{ji}\cdot d_j$ we obtain the algorithm
that turns a GDDM matrix into a s.d.d. by columns.

Once we have applied this transformation to the matrix at hand into a s.d.d.,
we can use the Monte Carlo linear solver which is ensured to converge.

\subsubsection{Block Diagonally Dominant}
\label{sec:bdd}

In the section we consider situations in which block diagonal preconditioning
can result in a convergent Monte Carlo linear solver.
By mirroring the computations we already showed previously, the iteration
matrix $H\in\mathbb{R}^{n\times n}$ resulting from a block diagonal
preconditioning takes the form
\[
 H=I-D^{-1}A=\begin{bmatrix}0_{n_1\times n_1} & -[A_{11}]^{-1}A_{12} & \cdots &
\cdots & -[A_{11}]^{-1}A_{1p} \\
-[A_{22}]^{-1}A_{21} & 0_{n_2\times n_2} & -[A_{22}]^{-1}A_{23} &
\cdots & -[A_{22}]^{-1}A_{2p}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
-[A_{pp}]^{-1}A_{p1} &  \cdots & \cdots&
-[A_{pp}]^{-1}A_{p,p-1} & 0_{n_p \times n_p}
\end{bmatrix}.
\]

For the sake of clarity, we note that the symbol "$\%$" is used in the following
subsections represents the modulus function between two integer numbers, while
the floor function is represented with the standard symbol "$\lfloor
\cdot \rfloor$".

We first consider the forward method.
By assuming that all $n_i$'s have the same value we may define
\[
 m=n_i=\text{size of a block}=\frac{n}{p}.
\]
The MAO transition probability matrix becomes:
\[
 P_{i,j}=\frac{\lvert H_{i,j} \rvert}{\sum_{k=1}^n\lvert H_{i,k}
\rvert}=\frac{\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor]
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg
\rvert}{\sum_{\substack{k=1\\k\ne i}}^n\bigg \lvert \bigg
([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{k}{m}\rfloor}\bigg )_{i\%m,k\%m}\bigg \rvert}
\]
Consequently, the $\hat{H}$ matrix is defined such that
\[
\hat{H}_{i,j} = \lvert H_{i,j}\rvert\bigg(\sum_{k=1}^n\lvert H_{ik}\rvert\bigg)=
\bigg \lvert \bigg ([A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg
\rvert
\sum_{\substack{k=1\\k\ne i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{k}{m}\rfloor}\bigg )_{i\%m,k\%m}\bigg \rvert
\]
By computing the sum over a generic row of $\hat{H}$ we get:
\[
 \sum_{j=1}^n \lvert \hat{H}_{i,j}\rvert=\sum_{j=1}^n \hat{H}_{i,j} =
 \bigg ( \sum_{\substack{j=1\\j\ne i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg \rvert \bigg ) ^2.
\]
If we focus on the norm $\lVert \hat{H}\rVert_{\infty}$, then the following
equivalence condition holds:
\[
 \lVert \hat{H}\rVert_{\infty}<1 \Leftrightarrow \sum_{\substack{j=1\\j\ne
i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg \rvert <1 \quad \forall
i=1,\cdots,n
\]
 A sufficient condition for this to happen is
 \begin{equation}
  \sum_{\substack{j=1\\j\ne i}}^p \lVert [A_{ii}]^{-1}A_{ij}\rVert_{\infty}<1.
    \label{block_cs}\quad \forall i=1,\ldots,p.
 \end{equation}
By defining a matrix $\tilde{H}\in \mathbb{R}^{p\times p}$ such that
\[
 \tilde{H}=\begin{bmatrix}0 & \lVert [A_{11}]^{-1}A_{12}\rVert_{\infty} &
\cdots &
\cdots & \lVert [A_{11}]^{-1}A_{1p}\rVert_{\infty} \\
\lVert [A_{22}]^{-1}A_{21}\rVert_{\infty} & 0 & \lVert
[A_{22}]^{-1}A_{23}\rVert_{\infty} &
\cdots & \lVert [A_{22}]^{-1}A_{2p}\rVert_{\infty} \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
\lVert [A_{pp}]^{-1}A_{p1}\rVert_{\infty} &  \cdots & \cdots&
\lVert [A_{pp}]^{-1}A_{p,p-1}\rVert_{\infty} & 0
\end{bmatrix}
\]
We can use the $\tilde{H}$ matrix just defined in order to introduce a
sufficient condition for the convergence of the forward Monet Carlo method with
a Block Diagonal preconditioning.
\begin{equation}
 \lVert \tilde{H} \rVert_{\infty}<1 \Rightarrow \lVert \hat{H}
\rVert_{\infty}<1.
\end{equation}

We now turn our attention to the adjoint method.
Analogously to the forward method, if we define
\[
(\hat{H})^T_{i,j} = \lvert H^T_{i,j}\rvert\bigg(\sum_{k=1}^n\lvert
H^T_{ik}\rvert\bigg)
\]
we can formulate a sufficient condition for the convergence of the adjoint
Monte
Carlo method by introducing a matrix $\tilde{H}$ which in this case is such that
\[
  \tilde{H}=\begin{bmatrix}0 & \lVert [A_{11}]^{-1}A_{12}\rVert_{1} & \cdots
&
\cdots & \lVert [A_{11}]^{-1}A_{1p}\rVert_{1} \\
\lVert [A_{22}]^{-1}A_{21}\rVert_{1} & 0 & \lVert
[A_{22}]^{-1}A_{23}\rVert_{1} &
\cdots & \lVert [A_{22}]^{-1}A_{2p}\rVert_{1} \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
\lVert [A_{pp}]^{-1}A_{p1}\rVert_{1} &  \cdots & \cdots&
\lVert [A_{pp}]^{-1}A_{p,p-1}\rVert_{1} & 0
\end{bmatrix}.
\]
The sufficient condition assumes the form
\begin{equation}
 \lVert \tilde{H} \rVert_{1}<1 \Rightarrow \lVert \hat{H} \rVert_{1}<1.
\end{equation}

\subsection{Adaptive Methods}

In formulas (\ref{dir_mean}) and (\ref{adj_mean}), the estimation of the
solution to
the linear system (\ref{linsys}) involves infinite sums. Of course this is
not viable in a computational setting,
therefore it is necessary to truncate both the number of steps taken in
a single random walk as well as the number of random walks that
are performed.

\subsubsection{History Length}

We first consider criteria to terminate an individual random walk, effectively
deciding how many terms of the Neumann series will be considered.
One possibility is to set a predetermined history length at which point all
histories are terminated.  The approach, however, presents two difficulties.
First, it is difficult to determine a priori how many steps on average will be
necessary to achieve a specified tolerance.  Second, due to the stochastic
nature of the random walks, some histories will retain important information
longer than others.  Truncating histories at a predetermined step runs the
risk of either prematurely truncating important histories, leading to larger
errors, or continuing unimportant histories longer than necessary,
leading to computational inefficiency.

Our goal is to apply a cutoff via an automatic procedure, without requiring
the intervention of the user directly.
Therefore we are looking for an integer $m$ such that
\[
\tilde{\theta}_i=E\bigg[\sum_{\ell=0}^m W_{\ell}
f_{k_{\ell}}\bigg]=x_i=\sum_{\ell=0}^m
\sum_{k_1=1}^{n}\sum_{k_2=1}^n\cdots \sum_{k_{\ell}=1}^n
P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},
k_{\ell}}w_{k_0,k_1}w_{k_1,k_2}\cdots
w_{k_{\ell-1}, k_{\ell}}f_{k_{\ell}}
\]
and
\[
\tilde{\theta}_j=E\bigg[\sum_{\ell=0}^m W_{\ell}\delta_{k_{\ell},
j}\bigg]=\sum_{\ell=0}^{m}\sum_{k_1}^n\sum_{k_2}^n\cdots\sum_{k_{\ell}}^n
f_{k_0}P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},K_{\ell}}w_{k_0,k_1}\cdots
w_{k_{\ell-1},k_{\ell}}\delta_{k_{\ell},j}
\]
are good approximations of (\ref{dir_mean}) and (\ref{adj_mean}) respectively.

In \cite{Slattery2013}, a criterion was used for the cutoff of the random walk
applicable to both the forward and the adjoint method. It requires the set up
of a relative weight cutoff threshold $W_c$ and to look for a step $m$ such that
\begin{equation}
W_m \le W_c W_0 \,,
\label{cutoff}
\end{equation}
where $W_0$ is the value of the weight at the initial step of the random walk
and $W_m$ is the value of the weight after $m$ steps.

\subsubsection{Number of Random Walks}

We now consider the selection of the number of random walks that should be
performed to achieve a given accuracy.  Unlike the termination of histories,
this is a subject that has not been discussed in the Monte Carlo linear
solver literature, as all previous studies have considered the simulation
of a prescribed number of histories.

The expression for the variance of the forward method is given by
formula (\ref{dir_var}).
In this context, a reasonable criterion to determine the number $\tilde{N}_i$
of random walks to be run is setting a threshold $\varepsilon_1$ and determine
\begin{equation}
\tilde{N}_i \quad s.t.\quad \frac{\sqrt{Var[\theta_i]}}{\lvert
E[\theta_i]\rvert}<\varepsilon_1, \quad i=1,\cdots,n.
\label{forward_adapt}
\end{equation}
The dependence of $Var[\theta_i]$ and $E[\theta_i]$ on $\tilde{N}_i$, which
seems
to be absent in the previous formula, is highlighted by the fact that
$\theta_i$
is estimated by fixing a finite number of histories.
Therefore, we are controlling the relative standard deviation requiring it not
to be too large. In other words, we are pursuing a statistical setting where the
uncertainty factor is small relative to the expected value.
This simple adaptive approach can be applied for the estimation of each
component $x_i$. Hence different number of histories may be employed to
compute different entries of the solution vector.

As concerns the adjoint method, the estimation of the variance for each entry
is reproduced by formula (\ref{adj_var}).
A possible adaptive selection of $\tilde{N}$, in this situation, is
\begin{equation}
\tilde{N} \quad s.t. \quad \frac{\lVert
\boldsymbol{\sigma}_{\tilde{N}}\rVert_1}{\lVert
\mathbf{x}\rVert_1}<\varepsilon_1,
\label{adjoint_adapt}
\end{equation}
where $\boldsymbol{\sigma}$ is a vector whose entries are
$\boldsymbol{\sigma}_{\tilde{N},i}=Var[\theta_i]$.

The criteria introduced above can be exploited in order to build
an a posteriori adaptive algorithm, capable of identifying the minimal value of
$\tilde{N}$ that verifies (\ref{forward_adapt}) or (\ref{adjoint_adapt})
respectively.
Algorithms \ref{alg:adaptive_for} and \ref{alg:adaptive_adj} describe
the Monte Carlo approaches with the adaptive criteria.

\begin{algorithm}[H]
 \KwData{$N$, $\varepsilon_1$}
 \KwResult{$\tilde{N}_i$, $\sigma_i$, $x_i$}
 For each entry of the solution vector (for $i=1,\cdots,n$) \;
 $\tilde{N}_i=N$\;
 compute $\theta_i$\;
 \While{$\frac{\sigma_i}{\lvert
E[\theta_i]\rvert}<\varepsilon_1$}{
\vspace{0.1cm}
  $\tilde{N}=\tilde{N}+N$\;
  compute $E[\theta_i]=x_i$\;
 }
 return $\theta_i$, $x_i$, $\sigma_i$\;
 \caption{A posteriori adaptive Forward Monte Carlo \label{alg:adaptive_for}}
\end{algorithm}

\begin{algorithm}[H]
 \KwData{$N$, $\varepsilon_1$}
 \KwResult{$\tilde{N}$, $\boldsymbol{\sigma_i}$, $\mathbf{x}$}
 $\tilde{N}=N$\;
 compute $\theta$\;
 \While{$\frac{\lVert\boldsymbol{\sigma} \rVert}{\lVert
E[\theta]\rVert}<\varepsilon_1$}{
\vspace{0.1cm}
  $\tilde{N}=\tilde{N}+N$\;
  compute $E[\theta]=\mathbf{x}$\;
 }
 return $\theta$, $\mathbf{x}$, $\boldsymbol{\sigma}$\;
 \caption{A posteriori adaptive Adjoint Monte Carlo \label{alg:adaptive_adj}}
\end{algorithm}

\subsection{Preconditioning}

Preconditioning can be incorporated into the Monte Carlo linear solver
algorithm by simply substituting $A \leftarrow P^{-1}A$ and
$b \leftarrow P^{-1}b$ in Eq.~\eqref{linsys}.  The Monte Carlo process, however,
introduced some constraints on the choice of preconditioners used.  Most
significantly, because the transition probabilities are built based on the
values of the iteration matrix, it is necessary to have access to the
entries of the preconditioned matrix $P^{-1}A$.
Therefore, we must focus on preconditioner selections that enable explicitly
forming the preconditioned matrix while retaining some of the sparsity of the
original matrix.

One possible preconditioning approach involves either diagonal or block
diagonal preconditioning.  Convergence of \ref{fixedpoint} with block
diagonal preconditioning, from the
deterministic point of view, is guaranteed when $A$ is an irreducibly
diagonally dominant matrix or when $A$ is an $M$-matrix. In fact in situation
such as these, theoretical results guarantee that $A=D-N$ is a regular
splitting (see \cite{Ax1996}). However the same theoretical results cannot be
exploited to guarantee the convergence of the MC solvers, because of the
additional check on $\rho(\hat{H})<1$ to be considered.
From the discussions in Section~\ref{sec:sdd}
and \ref{sec:bdd}, selecting a diagonal or block diagonal preconditioner can
guarantee convergence for matrices that are strictly diagonally dominant
or block diagonally dominant, respectively.

Other standard preconditioning approaches can also be used in an attempt
to reduce $\rho(\hat{H})$ while still retaining the ability to explicitly
form the preconditioned matrix.  One possibility is the use of incomplete
LU factorizations \cite{Saad,Benzi2002}.  If $P=LU$ is the preconditioner
with sparse triangular factors $L$ and $U$, then $P^{-1}A$ can easly be
formed by back substitution.
Variants of the algorithm set constraints on the amount of fill-in
of the sparsity pattern of the triangular factors, allowing the user to
find a compromise between sparsity and accuracy in
approximating the inverse of $A$.

Another class of preconditioners that are viable for use with Monte Carlo
linear solvers are approximate inverse preconditioners
\cite{Saad,Benzi2002,Tuma1996,Tuma1998}.  In these algorithms, an approximation
to the inverse of $A$ is generated directly, making computation of the
preconditioned matrix a straightforward task.  As with ILU factorizations,
multiple versions of approximate inverse preconditioning exist which may
have different behavior in terms of effectiveness of the preconditioner
versus the resulting reduction in sparsity.

A downside with the use of ILU or approximate inverse preconditioning is the
the quality of preconditioner needed to achieve $\rho(\hat{A})<1$ is
difficult to determine.  In fact, in some situations it may happen that
modifying the preconditioner to reduce $\rho(A)$ may actually lead to an
increase in $\rho(\hat{A})$, decreasing the effectiveness of the Monte
Carlo process on the system.


