\section{Convergence Behavior of Stochastic Methods}

The requests imposed on the expected value and second moment of the estimators
can be reformulated in a purely deterministic setting by looking at the
spectral radii of some properly defined matrices.

For instance the condition of finiteness of the expected value can be
reformulated by requiring
\begin{equation}
 \rho(H)<1
 \label{rho_h},
\end{equation}

where $H$ is the iteration matrix of the fixed point scheme.
In fact both equations (\ref{dir_mean}) and (\ref{adj_mean}) can be considered
as
power series in terms of $H$ and the spectral radius of $H$ smaller
than 1 is a necessary and sufficient condition for the Neumann series to
converge.


By analogy we aim at reproducing a similar reasoning for the second moment of
the estimator, since we want it to be finite as well.
By looking at the equations of the variance (\ref{dir_var}) and (\ref{adj_var})
for
the Forward and the Adjoint method respectively, we notice that the second
moment can be reinterpreted as a power series with respect to the matrices
defined as follows:

\[
 \hat{H}_{i,j}=\frac{H_{i,j}^2}{P_{i,j}} \quad \text{- Forward Method}.
\]

or

\[
 \hat{H}_{i,j}=\frac{H_{j,i}^2}{P_{i,j}} \quad \text{- Adjoint Method}.
\]


Accordingly to the necessary and sufficient condition for the power series to
converge, we require
\begin{equation}
 \rho(\hat{H})<1.
 \label{rho_star}
\end{equation}

Condition (\ref{rho_h}) must be required for a generic fixed point scheme to
reach convergence, whereas the extra condition (\ref{rho_star}) is typical of
the stochastic schemes presented in this work. Having two conditions to
verify makes the numerical treatment of the linear system via MC very
difficult. We will discuss this in details later in the treatise. \newline

\subsection{Necessary and Sufficient Conditions}

Here below some results presented in
\cite{MASC2013} about necessary conditions and sufficient
conditions for
convergence are enunciated. In particular a suitable choice for constructing a
transition probability is discussed to guarantee and speed-up convergence.

We remember that the construction of the transition probability must satisfy
some constraints such as

\[
\begin{cases}
  P_{i,j}\ge 0 \\
 \sum_{j=1}^N P_{i,j}=1
\end{cases}
\]

plus one more restriction which varies depending on the particular Monte Carlo
approach chosen

\begin{align*}
& \text{Forward Method:} \quad H_{i,j}\ne 0 \Rightarrow P_{i,j}\ne 0 \\ &
\text{Adjoint Method:} \quad H_{j,i}\ne 0 \Rightarrow P_{i,j}\ne 0
\end{align*}.

The previous restrictions on the attainable values for the entries of the
transition probability are called \textit{transition conditions}.


At first an introductory theoretical result is shown, necessary to motivate the
choices made afterwards for the choice of the transition probability.

\begin{thm}
 Consider a vector $a=(a_1,a_2,\cdots,a_N)^T$ where at least one element is
nonzero, $a_k\ne0$ for some $k\in\{1,\cdots,N\}$.
\begin{itemize}
 \item For a probability vector $p=(p_1,p_2,\cdots,p_N)^T$ satisfying the
transition conditions, the lower bound of $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}$ is $\bigg(\sum_{k=1}^N \lvert a_k\rvert\bigg)^2$
\item There always exists a probability vector such that $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}\ge c\ge 1$ for all $c>1$.
\end{itemize}
\label{lemma}
\end{thm}

It might be observed that the minimum of the quantity $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}$ is attained when the probability vector is
defined such as $\displaystyle p_k=\frac{\lvert a_k\rvert}{\sum_{k=1}^N \lvert
a_k\rvert}$. It implies that the $\infty$-norm of $\hat{H}$ is minimized as
much
as possible. \newline

For the sake of simplicity let us introduce now a generic random walk
truncated at a certain $k$-th step
\[
 \gamma_k:\; r_0\rightarrow r_1 \rightarrow r_2 \rightarrow \cdots \rightarrow
r_k
\]
and the statistical estimator
\[
 X(\gamma_k)=\frac{H_{r_0,r_1}H_{r_1,r_2}\cdots
H_{r_{k-1},r_k}}{P_{r_0,r_1}P_{r_1,r_2}\cdots P_{r_{k-1},r_k}}f_{r_k}
\]
which is associated with the Forward Method.

Then the following theorem holds
\begin{thm}\textit{(Suited to the Forward Method)}

Let $H\in \mathbb{R}^{n\times n}$ such that $\lVert H\rVert_{\infty}<1$.
Consider $\nu_k$ as the realization of a random walk $\gamma$ truncated at the
$k$-th step. Then,
there always exists a
transition matrix $P$ such that
$Var\Big(X(\nu_k)\Big)\rightarrow 0$ and
$Var\Big(\sum_{\nu}X(\nu_k)\Big)$ is bounded as $k\rightarrow \infty$.
\label{for_thm}
\end{thm}

If we introduce the estimator
\[
 X(\gamma_k)=\frac{H^T_{r_0,r_1}H^T_{r_1,r_2}\cdots
H^T_{r_{k-1},r_k}}{P_{r_0,r_1}P_{r_1,r_2}\cdots
P_{r_{k-1},r_k}}sign(f_{r_0})\lVert \mathbf{f}\rVert_1
\]
which is associated with the Adjoint Method, then we can state a
theorem specular to \ref{for_thm}.

\begin{thm}\textit{(Suited to the Adjoint Method)}

 Let $H\in \mathbb{R}^{n\times n}$ with $\lVert H\rVert_{1}<1$.
Consider $\nu_k$ as the realization of a random walk $\gamma$ truncated at the
$k$-th step. Then,
there always exists a
transition matrix $P$ such that
$Var\Big(X(\nu_k)\Big)\rightarrow 0$ and
$Var\Big(\sum_{\nu}X(\nu_k)\Big)$ is bounded as $k\rightarrow \infty$.
\label{adj_thm}
\end{thm}

In particular, relying on what guaranteed by Theorem \ref{lemma}, in both
Theorems \ref{for_thm} and \ref{adj_thm} the thesis holds by picking the MAO
transition probability.

These theoretical results represent sufficient conditions for the convergence
of the
Forward and Adjoint Monte Carlo and they can be easily checked.
However requiring $\lVert
H\rVert_{\infty}<1$ or $\lVert H\rVert_1<1$ may be too demanding.
Indeed, even if there exist some preconditioners able to guarantee this
condition (e.g. approximate inverse preconditioners with a proper value of
dropping
tolerance), it may entail to compromise the sparsity of the
iteration
matrix.

Thusly, in situations where the hypotheses of Theorems \ref{for_thm} and
\ref{adj_thm} are too demanding to be verified, the necessary and sufficient
condition
on the
spectral radius of $\hat{H}$ is the only criterion to take as reference.
Nonetheless this constraint consists in
computing the spectral radii of two matrices and the calculation
might be very costly. Moreover the computation of the spectral radii is
affected by resilience issues as well as
a deterministic solver for the resolution of
a linear system. Therefore the original difficulty is transferred without being
actually tackled.

\subsubsection{Forward MC}

\subsubsection{Adjoint MC with Collision Estimator}

\subsubsection{Adjoint MC with Expected-Value Estimator}

\subsection{Construction of Transition Probabilities}

The way the transition probability is defined is extremely important. In fact
in some circumstances it might determine whether the stochastic scheme
converges or not. Two very simple ways of defining it are listed here
below. These are the approaches adopted typically in literature.

\subsubsection{Uniform probabilities}
With this approach transition matrix $P$ is such that all the non-zero elements
in each row have equal probability of occurring.
\[
\text{Forward}:\;P_{i,j}=
\begin{cases}
0 \quad \quad \quad \qquad \qquad \qquad \text{if}\quad H_{i,j}=0 \\
\frac{1}{\#(\text{non-zeros in the row)}} \quad \text{if} \quad H_{i,j}\ne 0
\end{cases}\quad
\]
\[
\text{Adjoint}:\;P_{i,j}=
\begin{cases}
0 \quad \quad \quad \qquad \qquad \qquad \text{if}\quad H_{j,i}=0 \\
\frac{1}{\#(\text{non-zeros in the column)}} \quad \text{if} \quad H_{j,i}\ne 0
\end{cases}
\]
The Monte Carlo approach resorting to this definition of the transition matrix,
in accordance to \cite{AADBTW2005}, is called \textit{Uniform Monte Carlo} (UM).

\subsubsection{Weighted probabilities}
An alternative way of defining the transition matrix consists in attributing
probabilities to all the entries of the transition probability accordingly
to the magnitude of the elements. For instance we may employ the following
definition
\[
\text{Forward}: \;
p(k_i=j \;\lvert\;k_{i-1}=i )=P_{i,j}=\frac{\lvert
H_{i,j}\rvert^p}{\sum_{k=1}^n
\lvert H_{i,k}\rvert^p}
\]

\[
\text{Adjoint}: \;
p(k_i=j \;\lvert\;k_{i-1}=i )=P_{i,j}=\frac{\lvert
H_{j,i}\rvert^p}{\sum_{k=1}^n
\lvert H_{k,i}\rvert^p}
\]


where $p\in \mathbb{N}$.
This way of proceeding, for $p=1$, still basing on the work \cite{AADBTW2005},
is called
\textit{Monte Carlo Almost Optimal} (MAO).
The reason why the almost optimal probability is called this way is associated
with a notice from Theorem \ref{lemma}. Indeed from
the mentioned lemma we know that the quantity $\displaystyle
\sum_{k=0}^N\frac{a_k^2}{p_k}$ is minimized when the probability vector is
defined such as $\displaystyle p_k=\frac{\lvert a_k\rvert}{\sum_{k=1}^N \lvert
a_k\rvert}$. This implies that the almost optimal probability minimizes the
$\infty$-norm of $\hat{H}$ in the Forward method and the $1$-norm of $\hat{H}$
in the
Adjoint method. This is very important to be pointed out since these two norms
are so far the best practical upper bounds for $\rho(\hat{H})$ that we can
easily
handle. In the Section dedicated to the numerical results it will be proved
that the Almost Optimal Probability eases the convergence of the MC solver,
even in cases when the employment of the Uniform Probability does not provide
convergence at all.

\subsection{Classes of Matrices with Guaranteed Convergence}

As we have already seen, sufficient conditions for
the convergence of the Monte Carlo linear solvers are very restrictive (see
\cite{MASC2013}), whereas the necessary and sufficient condition requires the
computation of
$\rho(H)$and $\rho(\hat{H})$, which is very expensive in general. Once a
proper
preconditioner is picked (e.g. Approximate Inverse), $\rho(H)<1$ may
be always guaranteed, whilst $\rho(\hat{H})<1$ is more
problematic to be verified.

In order to bypass the cost of these checks, it is useful to pursue for sets of
matrices for
which the Monte Carlo linear solver always converges, maybe with the use of an
ad-hoc preconditioner.

\subsubsection{Strictly Diagonally Dominant}

 One of the these sets is
represented by strictly diagonally dominant matrices.

\begin{defn}
 A matrix $A\in\mathbb{R}^{n\times n}$is strictly diagonally dominant (s.d.d)
by rows if
 \begin{equation}
    \lvert a_{ij}\rvert>\sum_{\substack{i=1\\ i\ne j}}^{i=n}\lvert
a_{ij}\rvert
\label{sddr}
 \end{equation}
\end{defn}

\begin{defn}
 A matrix $A\in\mathbb{R}^{n\times n}$is strictly diagonally dominant (s.d.d)
by columns if
 \begin{equation}
    \lvert a_{ij}\rvert>\sum_{\substack{j=1\\ j\ne i}}^{j=n}\lvert
a_{ij}\rvert
\label{sddc}
 \end{equation}
\end{defn}

When (\ref{sddr}) holds we can resort to a left diagonal preconditioning and we
get an iteration matrix $H=I-D^{-1}A$ such that $\lVert H \rVert_{\infty}<1$.

Introducing a MAO transition probability for the Forward Method we get
\[
 P_{ij}=\frac{\lvert H_{ij}\rvert}{\sum_{k=1}^n\lvert H_{ik}\rvert}
\]
therefore the entries of $\hat{H}$ are defined as follows
\[
 \hat{H}_{ij}=\frac{H^2_{ij}}{P_{ij}}=\lvert H_{ij}\rvert\bigg
(\sum_{k=1}^n\lvert
H_{ik}\rvert\bigg ).
\]
Therefore
\[
 \sum_{j=1}^n\lvert \hat{H}_{ij} \rvert = \sum_{j=1}^n \hat{H}_{ij} =\bigg
(\sum_{j=1}^n \lvert H_{ij}\rvert\bigg )\bigg (\sum_{k=1}^n\lvert
H_{ik}\rvert\bigg ) =\bigg
(\sum_{j=1}^n \lvert H_{ij}\rvert\bigg ) ^2 <1 \quad \forall i=1,\cdots, n.
\]
This automatically implies that $\rho(\hat{H})<=\lVert
\hat{H}\rVert_{\infty}<1$. Thus the Forward Monte Carlo converges.
However nothing a priori can be
stated about the behavior of the Adjoint method.\newline

Instead if (\ref{sddc}) holds we can resort to a right diagonal preconditioning
and we
get an iteration matrix $H=I-AD^{-1}$ such that $\lVert H \rVert_{1}<1$.
In this case, by following a similar reasoning to the one made before, we
conclude that the Adjoint Method convergences thanks to $\lVert
\hat{H}\rVert_1<1$,
whereas nothing can be said in advance as concerns the Forward method.

\subsubsection{Generalized Diagonally Dominant}

 Another set of matrices for which the convergence of
the MC solvers is guaranteed is made of \textit{generalized diagonally
dominant matrices}.

\begin{defn}
A square matrix $A\in\mathbb{R}^{n\times n}$ is said to be generalized
diagonally dominant if
\[
 \lvert a_{ii}\rvert x_i \ge \sum_{\substack{j=1\\j\ne i}}^n \lvert
a_{ij}\rvert
x_j, \quad i=1,\cdots,n
\]
for some positive vector $\mathbf{x}=(x_1,\cdots,x_n)^T$.
\end{defn}

A proper subset of the generalized diagonally dominant matrices is represented
by $M$- matrices (see \cite{Ax1996}).
There are many definitions of $M$-matrices that may be found on books and all
of them are equivalent. The one presented in \cite{Saad} states the
following

\begin{defn}
A matrix $A\in\mathbb{R}^{n\times n}$ is said to be an $M$-matrix if it
satisfies the following four properties:
\begin{itemize}
 \item $a_{i,i}>0$ for $i=1,\cdots,n$
 \item $a_{i,j}\le 0$ for $i\ne j$, $i,j=1,\cdots,n$
 \item $A$ is nonsingular
 \item $A^{-1}\ge 0$
\end{itemize}
\end{defn}


A very important result about $M$-matrices enables to come up with a convergent
splitting and, therefore, a convergent fixed point scheme.

In particular Jacobi, Block Jacobi and Gauss Seidel  are
convergent
splitting for an $M$-matrix.
However, the construction of a splitting like this is not enough to ensure the
convergence of
the Monte Carlo linear solver. In fact none of these conditions is sufficient
to guarantee $\rho(\hat{H})<1$. Because of this obstacle, other options must
be considered.\newline

For this purpose we
appeal to a result in \cite{Li2002}. In this paper the author presents an
algorithm to transform a generalized diagonally dominant matrix with
nonzero diagonal entries into strictly diagonally dominant. The
algorithm works for a generic complex matrix $A\in\mathbb{C}^{n\times n}$.

Here below we report the algorithm at hand\newline

\begin{algorithm}[H]
 For a given complex matrix $A$, $a_{ii}\ne 0$, $i=1,\cdots, n$;\newline
 \begin{enumerate}
  \item Compute $S_{i}=\sum_{\substack{j=1 \\ j\ne i}^{n}}\lvert
a_{ij}\rvert$,
$i=1,2,\cdots,n$
\item Set $t=0$. For $i=1,2,\cdots, n$, if $\lvert a_{ii}\rvert>S_i$, then set
$t=t+1$
\item If $t=0$, then print "A is not a GDDM": END
\item If $t=n$, then print "A is a GDDM": END
\item \For{i=1,n}{$d_{i}=\frac{S_i+\varepsilon}{\lvert
a_{ii}\rvert+\varepsilon} \quad
\varepsilon>0,\quad j=1,2,\cdots,n$\;
$a_{ji}=a_{ji}\cdot d_i$}
\item Go to step 1.
 \end{enumerate}
 \caption{Algorithm to turn a GDDM matrix into a s.d.d by rows.}
\end{algorithm}

This approach turns a generalized diagonally dominant matrix into a
strictly diagonally dominant matrix by rows. By substituting
$\displaystyle S_{i}=\sum^{n}_{\substack{j=1 \\ j\ne i}}\lvert a_{ij}\rvert$
at step 1 with
$\displaystyle S_{i}=\sum^{n}_{\substack{i=1 \\ i\ne j}}\lvert a_{ij}\rvert$
and by replacing
$a_{ji}=a_{ji}\cdot d_i$ with $a_{ji}=a_{ji}\cdot d_j$ we obtain the algorithm
that turns a GDDM matrix into a s.d.d. by columns.\newline

Once we have applied this transformation to the matrix at hand into a s.d.d.,
we can use the Monte Carlo linear solver which is ensured to converge.

\subsubsection{Block Diagonally Dominant}

 In this section we wonder in which situations a block diagonal preconditioning
might come up with a convergent Monte Carlo linear solver.
By mimicking the computations we already showed previously, the iteration
matrix $H\in\mathbb{R}^{n\times n}$ resulting from a block diagonal
preconditioning takes the form

\[
 H=I-D^{-1}A=\begin{bmatrix}0_{n_1\times n_1} & -[A_{11}]^{-1}A_{12} & \cdots &
\cdots & -[A_{11}]^{-1}A_{1p} \\
-[A_{22}]^{-1}A_{21} & 0_{n_2\times n_2} & -[A_{22}]^{-1}A_{23} &
\cdots & -[A_{22}]^{-1}A_{2p}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
-[A_{pp}]^{-1}A_{p1} &  \cdots & \cdots&
-[A_{pp}]^{-1}A_{p,p-1} & 0_{n_p \times n_p}
\end{bmatrix}.
\]

For the sake of clarity, we notify that the symbol "$\%$" used in the following
subsections represents the modulus function between two integer numbers, while
the floor function is represented with pretty standard symbol "$\lfloor
\cdot \rfloor$".

\paragraph{Forward method}
By assuming that all $n_i$'s have the same value we may define
\[
 m=n_i=\text{size of a block}=\frac{n}{p}.
\]

The MAO transition probability matrix becomes:
\[
 P_{i,j}=\frac{\lvert H_{i,j} \rvert}{\sum_{k=1}^n\lvert H_{i,k}
\rvert}=\frac{\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor]
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg
\rvert}{\sum_{\substack{k=1\\k\ne i}}^n\bigg \lvert \bigg
([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{k}{m}\rfloor}\bigg )_{i\%m,k\%m}\bigg \rvert}
\]

Consequently, the $\hat{H}$ matrix is defined such that
\[
\hat{H}_{i,j} = \lvert H_{i,j}\rvert\bigg(\sum_{k=1}^n\lvert H_{ik}\rvert\bigg)=
\bigg \lvert \bigg ([A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg
\rvert
\sum_{\substack{k=1\\k\ne i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{k}{m}\rfloor}\bigg )_{i\%m,k\%m}\bigg \rvert
\]

By computing the sum over a generic row of $\hat{H}$ we get:
\[
 \sum_{j=1}^n \lvert \hat{H}_{i,j}\rvert=\sum_{j=1}^n \hat{H}_{i,j} =
 \bigg ( \sum_{\substack{j=1\\j\ne i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg \rvert \bigg ) ^2.
\]

If we focus on the norm $\lVert \hat{H}\rVert_{\infty}$, then the following
equivalence condition holds:
\[
 \lVert \hat{H}\rVert_{\infty}<1 \Leftrightarrow \sum_{\substack{j=1\\j\ne
i}}^n\bigg \lvert \bigg ([A_{\lfloor
\frac{i}{m}\rfloor \lfloor
\frac{i}{m}\rfloor}]^{-1} A_{\lfloor \frac{i}{m}\rfloor \lfloor
\frac{j}{m}\rfloor}\bigg )_{i\%m,j\%m}\bigg \rvert <1 \quad \forall
i=1,\cdots,n
\]
 A sufficient condition for this to happen is
 \begin{equation}
  \sum_{\substack{j=1\\j\ne i}}^p \lVert [A_{ii}]^{-1}A_{ij}\rVert_{\infty}<1.
    \label{block_cs}\quad \forall i=1,\ldots,p.
 \end{equation}

By defining a matrix $\tilde{H}\in \mathbb{R}^{p\times p}$ such that
\[
 \tilde{H}=\begin{bmatrix}0 & \lVert [A_{11}]^{-1}A_{12}\rVert_{\infty} &
\cdots &
\cdots & \lVert [A_{11}]^{-1}A_{1p}\rVert_{\infty} \\
\lVert [A_{22}]^{-1}A_{21}\rVert_{\infty} & 0 & \lVert
[A_{22}]^{-1}A_{23}\rVert_{\infty} &
\cdots & \lVert [A_{22}]^{-1}A_{2p}\rVert_{\infty} \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
\lVert [A_{pp}]^{-1}A_{p1}\rVert_{\infty} &  \cdots & \cdots&
\lVert [A_{pp}]^{-1}A_{p,p-1}\rVert_{\infty} & 0
\end{bmatrix}
\]

We can use the $\tilde{H}$ matrix just defined in order to introduce a
sufficient condition for the convergence of the Forward Monet Carlo method with
a Block Diagonal preconditioning.

\begin{equation}
 \lVert \tilde{H} \rVert_{\infty}<1 \Rightarrow \lVert \hat{H}
\rVert_{\infty}<1.
\end{equation}

\paragraph{Adjoint method}
Analogously to the Forward method, if we define
\[
(\hat{H})^T_{i,j} = \lvert H^T_{i,j}\rvert\bigg(\sum_{k=1}^n\lvert
H^T_{ik}\rvert\bigg)
\]
we can formulate a sufficient condition for the convergence of the Adjoint
Monte
Carlo method by introducing a matrix $\tilde{H}$ which in this case is such that
\[
  \tilde{H}=\begin{bmatrix}0 & \lVert [A_{11}]^{-1}A_{12}\rVert_{1} & \cdots
&
\cdots & \lVert [A_{11}]^{-1}A_{1p}\rVert_{1} \\
\lVert [A_{22}]^{-1}A_{21}\rVert_{1} & 0 & \lVert
[A_{22}]^{-1}A_{23}\rVert_{1} &
\cdots & \lVert [A_{22}]^{-1}A_{2p}\rVert_{1} \\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
\lVert [A_{pp}]^{-1}A_{p1}\rVert_{1} &  \cdots & \cdots&
\lVert [A_{pp}]^{-1}A_{p,p-1}\rVert_{1} & 0
\end{bmatrix}.
\]
The sufficient condition assumes the form
\begin{equation}
 \lVert \tilde{H} \rVert_{1}<1 \Rightarrow \lVert \hat{H} \rVert_{1}<1.
\end{equation}

\subsection{Adaptive Methods}

In formulas (\ref{dir_mean}) and (\ref{adj_mean}) the estimation of the
solution to
the linear system (\ref{linsys_prec}) involves infinite sums. Of course this is
not viable for
computational issues, therefore it is necessary to resort to truncation of
the sums through proper criteria. In details, the truncation implies the
employment of a finite number of permutations and a finite number of
steps for each one of the permutations computed. Therefore it is necessary to
truncate at two different levels to decide both how many permutations to employ
and how many steps each one of them should be kept running.

\subsubsection{History Length}

At first we focus on the definition of a weight cut-off
in order to decide where to terminate the histories. It may be reinterpreted as
a quantity guiding us to decide how many terms of the Neumann
series to consider. \newline

The goal is to apply the cutoff via an automatic procedure, without requiring
the intervention of the user directly.
Therefore we are looking for an integer $m$ such that
\[
\tilde{\theta}_i=E\bigg[\sum_{\ell=0}^m W_{\ell}
f_{k_{\ell}}\bigg]=x_i=\sum_{\ell=0}^m
\sum_{k_1=1}^{n}\sum_{k_2=1}^n\cdots \sum_{k_{\ell}=1}^n
P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},
k_{\ell}}w_{k_0,k_1}w_{k_1,k_2}\cdots
w_{k_{\ell-1}, k_{\ell}}f_{k_{\ell}}
\]

and

\[
\tilde{\theta}_j=E\bigg[\sum_{\ell=0}^m W_{\ell}\delta_{k_{\ell},
j}\bigg]=\sum_{\ell=0}^{m}\sum_{k_1}^n\sum_{k_2}^n\cdots\sum_{k_{\ell}}^n
f_{k_0}P_{k_0,k_1}P_{k_1,k_2}\cdots P_{k_{\ell-1},K_{\ell}}w_{k_0,k_1}\cdots
w_{k_{\ell-1},k_{\ell}}\delta_{k_{\ell},j}
\]

are good approximations of (\ref{dir_mean}) and (\ref{adj_mean}) respectively.


In \cite{Slattery2013} there is a criterion for the cutoff of the random walk
applicable to both the Forward and the Adjoint method. It requires the set up
of a relative cutoff threshold $W_c$ and to look for a step $m$ such that
\begin{equation}
W_m \le W_c W_0.
\label{cutoff}
\end{equation}
$W_0$ is the value of the weight at the initial step of the random walk and
$W_m$ is the value of the weight after $m$ steps.

\subsubsection{Number of Random Walks}

We now focus on quantities that might lead to a proper
number of permutations to compute, in order to get a prescribed order of
accuracy.
As regards the Forward method, we know that the expression for the variance
is defined in formula (\ref{dir_var}).

In this context, a reasonable criterion to determine the number $\tilde{N}_i$
of random walks to be run is setting a threshold $\varepsilon_1$ and determine
\begin{equation}
\tilde{N}_i \quad s.t.\quad \frac{\sqrt{Var[\theta_i]}}{\lvert
E[\theta_i]\rvert}<\varepsilon_1, \quad i=1,\cdots,n.
\label{forward_adapt}
\end{equation}
The dependence of $Var[\theta_i]$ and $E[\theta_i]$ on $\tilde{N}_i$, which
seems
to be absent in the previous formula, is highlighted by the fact that
$\theta_i$
is estimated by fixing a finite number of histories.
Therefore we are controlling the relative standard deviation requiring it not
to be too large. In other words we are pursuing a statistical setting where the
uncertainty factor is not dominating over the expected value.
This simple adaptive approach can be applied for the estimation of each
component $x_i$. Hence different number of histories may be employed to
compute different entries of the solution vector. \newline


As concerns the Adjoint method, the estimation of the variance for each entry
is reproduced by Formula (\ref{adj_var}).

A possible adaptive selection of $\tilde{N}$, in this situation, is
\begin{equation}
\tilde{N} \quad s.t. \quad \frac{\lVert
\boldsymbol{\sigma}_{\tilde{N}}\rVert_1}{\lVert
\mathbf{x}\rVert_1}<\varepsilon_1,
\label{adjoint_adapt}
\end{equation}
where $\boldsymbol{\sigma}$ is a vector whose entries are
$\boldsymbol{\sigma}_{\tilde{N},i}=Var[\theta_i]$.

What introduced above can be exploited in order to build
an a posteriori adaptive algorithm, capable to identify the minimal value of
$\tilde{N}$ that verifies (\ref{forward_adapt}) or (\ref{adjoint_adapt})
respectively.

Below there are the pseudo-codes associated with both Forward and Adjoint Monte
Carlo.

\begin{algorithm}[H]
 \KwData{$N$, $\varepsilon_1$}
 \KwResult{$\tilde{N}_i$, $\sigma_i$, $x_i$}
 For each entry of the solution vector (for $i=1,\cdots,n$) \;
 $\tilde{N}_i=N$\;
 compute $\theta_i$\;
 \While{$\frac{\sigma_i}{\lvert
E[\theta_i]\rvert}<\varepsilon_1$}{
\vspace{0.1cm}
  $\tilde{N}=\tilde{N}+N$\;
  compute $E[\theta_i]=x_i$\;
 }
 return $\theta_i$, $x_i$, $\sigma_i$\;
 \caption{A posteriori adaptive Forward Monte Carlo}
\end{algorithm}

\begin{algorithm}[H]
 \KwData{$N$, $\varepsilon_1$}
 \KwResult{$\tilde{N}$, $\boldsymbol{\sigma_i}$, $\mathbf{x}$}
 $\tilde{N}=N$\;
 compute $\theta$\;
 \While{$\frac{\lVert\boldsymbol{\sigma} \rVert}{\lVert
E[\theta]\rVert}<\varepsilon_1$}{
\vspace{0.1cm}
  $\tilde{N}=\tilde{N}+N$\;
  compute $E[\theta]=\mathbf{x}$\;
 }
 return $\theta$, $\mathbf{x}$, $\boldsymbol{\sigma}$\;
 \caption{A posteriori adaptive Adjoint Monte Carlo}
\end{algorithm}

\subsection{Preconditioning}

In this section we focus on the pursuit of preconditioners that facilitate the
convergence of the stochastic schemes introduced so far. A priori, all of the
following techniques are good candidates to improve the numerical setting.
However issues about the fill-in effect and the verification of convergence
conditions limit their employment for our purposes.

\subsubsection*{Block Preconditioning}
Consider a matrix $A\in\mathbb{R}^{n\times n}$ of the form
\[
 A=\begin{bmatrix}A_{11} & A_{12} & \cdots & A_{1p}\\
    A_{21} & A_{22} & \cdots & A_{2p}\\
    \vdots & \vdots & \ddots & \vdots \\
    A_{p1} & \cdots & \cdots & A_{pp}
   \end{bmatrix}
\]
where $p$ is a divisor of $n$ and such that $A_{ii} \in \mathbb{R}^{n_i
\times n_i}$ are nonsingular square matrices.
By defining a block diagonal matrix such as
\[
 D=\begin{bmatrix}A_{11} & 0_{12} & \cdots & 0_{1p}\\
    0_{21} & A_{22} & \cdots & 0_{2p}\\
    \vdots & \vdots & \ddots & \vdots \\
    0_{p1} & \cdots & \cdots & A_{pp}
   \end{bmatrix}
\]
it is possible to compute its inverse.
$D^{-1}$  can be used as a preconditioner for the original matrix $A$.
In particular
\[
 D^{-1}A=\begin{bmatrix}I_{n_1\times n_1} & A_{11}^{-1}A_{12} & \cdots &
\cdots & A_{11}^{-1}A_{1p} \\
A_{22}^{-1}A_{21} & I_{n_2\times n_2} & A_{22}^{-1}A_{23} &
\cdots & A_{22}^{-1}A_{2p}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\vdots & \vdots & \vdots &\ddots & \vdots \\
A_{pp}^{-1}A_{p1} &  \cdots & \cdots&
A_{pp}^{-1}A_{p,p-1} & I_{n_p \times n_p}
\end{bmatrix}.
\]
The ultimate goal is to resort to this preconditioning technique in order to
achieve the condition $\rho(D^{-1}A)<1$. In fact this is necessary in order to
reformulate \ref{linsys} into \ref{fixedpoint}.

Convergence of \ref{fixedpoint} with block diagonal preconditioning, from the
deterministic point of view, is guaranteed when $A$ is an irreducibly
diagonally dominant matrix or when $A$ is an $M$-matrix. In fact in situation
such as these, theoretical results guarantee that $A=D-N$ is a regular
splitting (see \cite{Ax1996}). However the same theoretical results cannot be
exploited to guarantee the convergence of the MC solvers, because of the
additional check on $\rho(\hat{H})<1$ to be considered.

\subsubsection*{Approximate Inverse of a Sparse Matrix (AINV)}

This stimulated our interests into other types of preconditioners. At first we
concentrated on Approximate Inverse preconditioners (see \cite{Saad} and
\cite{Benzi2002}).

There are many practical techniques that may be applied in order to build the
AINV preconditioner.
One of these is described in \cite{Tuma1996} and \cite{Tuma1998}.
This is the actual approach we applied to the construction of the
preconditioner. The technique constructs the preconditioner via a
bi-conjugation fashion.
The idea is to compute an approximate factorization of the form $W^T A Z=D$.
$W$ and $Z$ are unit upper triangular matrices and $D$ is a diagonal matrix.

In order to compute the AINV preconditioner, a \texttt{FORTRAN} code
provided by Miroslav Tuma (Academy of Sciences, Prague) is used.
We report as follows the algorithm employed for the construction of the so
called \textit{Right-looking Factored AINV}. The algorithm provide the user
with a dropping tolerance $\tau$ which can be used in order to tune the fill-in
effect
of the factored preconditioner. \newline

\begin{algorithm}[H]
 \KwData{$p=q=(0,\cdots,0)\in \mathbb{R}^n$, $Z=[z_1,\cdots,z_n]=I_n$,
$W=[w_1,\cdots,w_n]=I_n$}
 \KwResult{$Z$, $W$}
 \For{$k=1,\cdots, n$}{
 $p_k=w_k^T A e_k$, $q_k=e_k^T A z_k$ \;
     \For{$i=k+1,\cdots,n$}{
         $p_i=(w_k^T A e_k)/p_k$, $q_i=(e_k^T A z_i)/q_k$\;
         Apply a dropping rule to $p_i$, $q_i$\;
         $w_i=w_i - w_k p_i$, $z_i=z_i - z_k q_i$\\
         Apply a dropping rule to $w_{j,i}$ and $z_{j,i}$ for $j=1,\cdots,i$\;
     }\;
  }\;
 Choose diagonal entries of $D$ as the component of $p$ or $q$\;
 return $Z$, $W$\;
 \caption{Right-looking Factored AINV}
\end{algorithm}

\subsubsection*{Incomplete LU Factorization Preconditioners (ILU)}
This set of preconditioners has the goal of computing a sparse lower triangular
matrix $L$ and a sparse upper triangular matrix $U$ such that the residual
matrix $R=LU-A$ respects some constraints. One of these constraints may be the
fact that some entries must be set to zero.
For instance we can introduce a zero pattern set
\[
 P\subset \{(i,j)\lvert i\ne j; 1\le i,j\le n\}
\]
and use this set in order to impose some constraints on the shape of the
preconditioner built.\newline

Different variants of the
Incomplete LU Factorization algorithm are available (see \cite{Saad}).
However it is
possible to prove that if all the formulation are well defined, then they are
all equivalent.
One of these variants is the following

\begin{algorithm}[H]
 \KwData{$P$}
 \KwResult{$A$}
 For each $(i,j)\in P$ set $a_{ij}=0$\;
 \For{$k=1,\cdot,n-1$}{
 \For{$i=k+1,n$}{
 \If{$(i,k)\notin P$}{
 $a_{ik}=\frac{a_{ik}}{a_{kk}}$
 \For{$j=k+1,\cdots, n$ and for $(i,j)\notin P$}{
  $a_{ij}=a_{ij}-a_{ik}*a_{kj}$
 }
 }
 }
 }
 return $Z$, $W$\;
 \caption{General Static Pattern ILU}
\end{algorithm}

This kind of algorithm is guaranteed to terminate without any breakdown just
for $M$-matrices.

Assuming that the zero pattern $P$ coincides with the zero pattern of $A$ leads
to the $ILU(0)$. \newline

\begin{algorithm}[H]
 \KwData{$P$}
 \KwResult{$A$}
 \For{$i=2,\cdot,n$}{
 \For{$k=i,i-1$ and for $(i,k)\in NZ(A)$}{
  $a_{ik}=\frac{a_{ik}}{a_{kk}}$
  \For{$j=k+1,\cdots,n$ and for $(i,j)\in NZ(A)$}{
  $a_{ij}=a_{ij}-a_{ik}a_{kj}$}
 }
 }
 \caption{ILU(0).}
\end{algorithm}

More sophisticated variants of the algorithm enable to enlighten the constraint
of the sparsity pattern, finding a compromise between sparsity and accuracy in
approximating the inverse of $A$. We refer to \cite{Saad} and
\cite{Benzi2002} for further information about it.
i
