\documentclass{beamer}
\usetheme{Warsaw}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{listings}
\usepackage{color}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage{tmadd,tmath}
\usepackage[mathcal]{euscript} 
\usepackage{color}
\usepackage{textcomp}
\usepackage{algorithm,algorithmic}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
  backgroundcolor=\color{lbcolor},
  tabsize=4,
  rulecolor=,
  language=c++,
  basicstyle=\scriptsize,
  upquote=true,
  aboveskip={1.5\baselineskip},
  columns=fixed,
  showstringspaces=false,
  extendedchars=true,
  breaklines=true,
  prebreak =
  \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
  frame=single,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  identifierstyle=\ttfamily,
  keywordstyle=\color[rgb]{0,0,1},
  commentstyle=\color[rgb]{0.133,0.545,0.133},
  stringstyle=\color[rgb]{0.627,0.126,0.941},
}

%% colors
\setbeamercolor{boxheadcolor}{fg=white,bg=black}
\setbeamercolor{boxbodycolor}{fg=black,bg=white}

%% slide numbers

%%---------------------------------------------------------------------------%%
\author[Stuart Slattery]{Stuart Slattery\\
  \bigskip
  \href{mailto:slatterysr@ornl.gov}{\texttt{slatterysr@ornl.gov}} \\
  \bigskip
  Oak Ridge National Laboratory}

\date{\today} 
\title[MRB Monte Carlo \hspace{1mm}
  \insertframenumber/\inserttotalframenumber]{Minimized-Residual Batch
Monte Carlo}
\begin{document}
\maketitle

%%---------------------------------------------------------------------------%%
\begin{frame}{Batched Mean Calculation}

  Consider the mean calculation for a Monte Carlo tally with
  observations $\hat{\ve{x}}$ with $N$ samples for linear problem
  $\ve{A}\ve{x}=\ve{f}$:
  \[
  \ve{x} = \frac{1}{N} \sum_{m=1}^N \hat{\ve{x}}_m
  \]

  We could alternatively arrive at the same value with $B$ batches of
  samples and $N_B$ sample per batch:
  \[
  \ve{x} = \frac{1}{B} \sum_{b=1}^B \frac{1}{N_B} \sum_{m=1}^{N_B}
  \hat{\ve{x}}_{b,m}
  \]

  This can also be written in terms of the batch result:
  \[
  \ve{x}_b=\frac{1}{N_B} \sum_{m=1}^{N_B} \hat{\ve{x}}_{b,m}
  \]
  \[
  \ve{x} = \frac{1}{B}\sum_{b=1}^B \ve{x}_b
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Batch Residual Minimization}

  Batch combination is then simply:
  \[
  \ve{x} = \sum_{b=1}^B \alpha_b \ve{x}_b
  \]
  with $\alpha_b = 1/B$ the typical case. \textbf{Maybe there are
    better choices for $\alpha_b$.} Solve a linearly constrained
  optimization problem:
  \[
  minimize\ ||\ve{f} - \ve{A}\ve{x}||_2
  \]
  such that:
  \[
  \sum_{b=1}^B \alpha_b = 1
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Linear Least Squares: Method of Elimination}

  Build the basis, $\ve{V} \in \mathbb{R}^{J \times B}$ (with $\ve{A}
  \in \mathbb{R}^{J \times J}$ and $\ve{x} \in \mathbb{R}^{J}$):
  \[
  \ve{V} = 
  \begin{bmatrix}
    (\ve{A}\ve{x}_0)_0 & (\ve{A}\ve{x}_1)_0 & \cdots & (\ve{A}\ve{x}_B)_0 \\
    (\ve{A}\ve{x}_0)_1 & (\ve{A}\ve{x}_1)_1 & \cdots & (\ve{A}\ve{x}_B)_1 \\
    (\ve{A}\ve{x}_0)_2 & (\ve{A}\ve{x}_1)_2 & \cdots & (\ve{A}\ve{x}_B)_2 \\
    \vdots             & \vdots            & \ddots & \vdots \\
    (\ve{A}\ve{x}_0)_J & (\ve{A}\ve{x}_1)_J & \cdots & (\ve{A}\ve{x}_B)_J \\
  \end{bmatrix}
  \]
  Incorporate the linear constraint through substitution:
  \[
  \ve{x} = \ve{x}_B + \sum_{b=1}^{B-1} \beta_b (\ve{x}_{b+1} - \ve{x}_b)
  \]
  
\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Linear Least Squares: Method of Elimination}

  Build the modified basis $\ve{Z} = \ve{V}\ve{W}$ with $\ve{W} \in
  \mathbb{R}^{B \times B-1}$:
  \[
  \ve{W} =
  \begin{bmatrix}
    -1 &    & & & & \\
     1 & -1 & & & & \\
       &  1 & -1 & & & \\
       &    & \ddots   & \ddots &  & \\
     & & & & 1 & -1 \\
    & & & & & 1 \\
  \end{bmatrix}
  \]
  Solve the normal equations for $\boldsymbol{\beta} \in
  \mathbb{R}^{B-1 \times 1}$ (e.g. QR factorization)
  \[
  \ve{Z}^T \ve{Z} \boldsymbol{\beta} = \ve{Z}^T ( \ve{f} - \ve{A}\ve{x}_B
  )
  \]
  Then extract the coefficients $\boldsymbol{\alpha} =
  \ve{W}\boldsymbol{\beta}$ with:
  \[
  \boldsymbol{\alpha} =
  \begin{bmatrix}
    \alpha_1 & \alpha_2 & \cdots & \alpha_{B-1} & 1 + \alpha_{B}
  \end{bmatrix}^T
  \]

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Numerical Experiments}

  Model problem with 1-D cell-centered finite difference:
  \[
  \nabla^2 \ve{x} - D \ve{x} = \ve{f}
  \]

  Things we can vary:
  \begin{itemize}
    \item $D$
    \item $\ve{f}$
    \item Grid size, $J$
    \item Total number of samples, $N$
    \item Number of batches, $B$
    \item Samples per batch, $N_B = N/B$
  \end{itemize}
  
  Fix $J=100$ $N_p=1000$ for now.

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Numerical Experiments: Case 1}

  \[
  f_i = 1.0\ \forall\ i \in [0,100]
  \]

  {\tiny
    \begin{table}[htpb!]
      \begin{center}
        \begin{tabular}{cccccccc}\hline\hline
          \multicolumn{1}{c}{} & \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{}& \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{$D$} & \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{}& \multicolumn{1}{c}{} \\
          & & \textbf{0} & \textbf{0.01} & \textbf{0.1} & \textbf{1} & \textbf{10} & \textbf{100} \\
          & \textbf{1} & 8.4834E+00 & 1.8970E+00 & 7.0310E-01 & 3.9193E-01 & 3.2879E-01 &
          2.7241E-01 \\
          & \textbf{2} & 8.4778E+00 & 1.8954E+00 & 6.9072E-01 & 3.9192E-01 & 3.2399E-01 &
          2.7199E-01 \\
          & \textbf{5} & 8.2309E+00 & 1.8789E+00 & 6.6844E-01 & 3.7606E-01 & 3.1277E-01 &
          2.6687E-01 \\
          $B$ & \textbf{10} & 7.6602E+00 & 1.7883E+00 & 6.4907E-01 & 3.6300E-01 & 3.0760E-01 &
          2.5714E-01 \\
          & \textbf{25} & 6.5961E+00 & 1.6245E+00 & 5.6745E-01 & 3.3102E-01 & 2.9362E-01 &
          2.4874E-01 \\
          & \textbf{50} & 5.4466E+00 & 1.1605E+00 & 4.3782E-01 & 2.7181E-01 & 2.5802E-01 &
          1.9530E-01 \\
          & \textbf{100} & 1.4047E-02 & 9.3756E-03 & 1.0033E-02 & 7.6760E-03 & 4.5450E-03 &
          5.9411E-04 \\
          \hline\hline
        \end{tabular}
      \end{center}
    \end{table}
  }

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Numerical Experiments: Case 2}

  \[
  f_i = 0.0\ \forall\ i \in [0,60],\ 
  f_i = 1.0\ \forall\ i \in (40,60),\ 
  f_i = 0.0\ \forall\ i \in [60,100]
  \]

  {\tiny
    \begin{table}[htpb!]
      \begin{center}
        \begin{tabular}{cccccccc}\hline\hline
          \multicolumn{1}{c}{} & \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{}& \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{$D$} & \multicolumn{1}{c}{}&
          \multicolumn{1}{c}{}& \multicolumn{1}{c}{} \\
          & & \textbf{0} & \textbf{0.01} & \textbf{0.1} & \textbf{1} &
          \textbf{10} & \textbf{100} \\
& \textbf{1} & 4.8790E+00 & 1.1028E+00 & 3.4098E-01 & 1.7219E-01 & 1.1424E-01 &
1.2893E-01 \\
& \textbf{2} & 4.8510E+00 & 1.1020E+00 & 3.3968E-01 & 1.5212E-01 & 1.1119E-01 &
1.2861E-01 \\
& \textbf{5} & 4.5341E+00 & 8.4308E-01 & 3.0910E-01 & 1.5648E-01 & 6.7866E-02 &
1.2861E-01 \\
$B$ & \textbf{10} & 4.3214E+00 & 6.1069E-01 & 2.3996E-01 & 1.0337E-01 & 5.7924E-02 &
1.0099E-01 \\
& \textbf{25} & 3.7743E+00 & 4.7162E-01 & 8.5319E-02 & 4.0885E-02 & 4.1795E-03 &
3.5761E-04 \\
& \textbf{50} & 2.3465E+00 & 3.1158E-01 & 6.7653E-03 & 5.6435E-03 & 4.1795E-03 &
3.5761E-04 \\
& \textbf{100} & 3.1621E-03 & 4.2125E-03 & 5.5546E-03 & 5.6435E-03 & 4.1795E-03 &
3.5761E-04 \\
          \hline\hline
        \end{tabular}
      \end{center}
    \end{table}
  }

\end{frame}

%%---------------------------------------------------------------------------%%
\begin{frame}{Summary}

  \begin{itemize}
  \item Extremely effective for problems with a local residual (RHS)
  \item Not as effective for problems with a diffuse global residual (RHS)
  \item Need enough batches to capture solution structure to get good
    performance.
  \item Better performance with larger numbers of batches
  \item Need to store a solution vector for every batch: MRBMC(N)
  \item Improvement in residual observed to typically coincide with
    improvement in error - variance reduction
  \end{itemize}

\end{frame}

%%---------------------------------------------------------------------------%%

\end{document}
