@techreport{cappello09toward,
  author        = "Franck Cappello and Al Geist and Bill Gropp and Laxmikant V.  Kale and William Kramer and Marc Snir",
  title         = "Toward Exascale Resilience",
  institution   = "University of Illinois at Urbana-Champaign (UIUC) - Institut National de Recherche en Informatique et en Automatique (INRIA) Joint Laboratory on PetaScale Computing",
  number        = "TR-JLPC-09-01",
  month         = jun,
  year          = "2009",
  url           = "http://institutes.lanl.gov/resilience/docs/Toward\%20Exascale\%20Resilience.pdf"
}

@techreport{elnozahy08system,
  author        = "Mootaz Elnozahy and Ricardo Bianchini and Tarek El-Ghazawi and Armando Fox and Forest Godfrey and Adolfy Hoisie and Kathryn McKinley and Rami Melhem and James Plank and Partha Ranganathan and Josh Simons",
  title         = "System Resilience at Extreme Scale",
  institution   = "Defense Advanced Research Project Agency (DARPA)",
  year          = "2008",
  url           = "http://institutes.lanl.gov/resilience/docs/Toward\%20Exascale\%20Resilience.pdf"
}

@techreport{geist09major,
  author        = "Al Geist and Robert F. Lucas",
  title         = "Major Computer Science Challenges at Exascale",
  institution   = "International Exascale Software Project",
  month         = feb,
  year          = "2009",
  url           = "http://www.exascale.org/mediawiki/images/8/87/ExascaleSWChallenges-Geist_Lucas.pdf",
  note          = "Whitepaper"
}

@techreport{kogge08exascale,
  author        = "Peter Kogge and Keren Bergman and Shekhar Borkar and Dan Campbell and William Carlson and William Dally and Monty Denneau and Paul Franzon and William Harrod and Kerry Hill and Jon Hiller and Sherman Karp and Stephen Keckler and Dean Klein and Robert Lucas and Mark Richards and Al Scarpelli and Steven Scott and Allan Snavely and Thomas Sterling and R. Stanley Williams and Katherine Yelick",
  title         = "{ExaScale} Computing Study: {Technology} Challenges in Achieving Exascale Systems",
  institution   = "Defense Advanced Research Project Agency (DARPA) Information Processing Techniques Office (IPTO)",
  year          = "2008",
  url           = "http://users.ece.gatech.edu/~mrichard/ExascaleComputingStudyReports/exascale_final_report_100208.pdf"
}

@conference{schroeder07understanding,
  author        = "Bianca Schroeder and Garth A. Gibson",
  title         = "Understanding Failures in Petascale Computers",
  booktitle     = "Journal of Physics: Proceedings of the Scientific Discovery through Advanced Computing Program ({SciDAC}) Conference 2007",
  volume        = "78",
  pages         = "2022--2032",
  month         = jun # "~24-28, ",
  year          = "2007",
  address       = "Boston, MA, USA",
  publisher     = PUBLISHER_PHYSICS,
  url           = "http://www.iop.org/EJ/abstract/1742-6596/78/1/012022",
  abstract      = "With petascale computers only a year or two away there is a pressing need to anticipate and compensate for a probable increase in failure and application interruption rates. Researchers, designers and integrators have available to them far too little detailed information on the failures and interruptions that even smaller terascale computers experience. The information that is available suggests that application interruptions will become far more common in the coming decade, and the largest applications may surrender large fractions of the computer's resources to taking checkpoints and restarting from a checkpoint after an interruption. This paper reviews sources of failure information for compute clusters and storage systems, projects failure rates and the corresponding decrease in application effectiveness, and discusses coping strategies such as application-level checkpoint compression and system level process-pairs fault-tolerance for supercomputing. The need for a public repository for detailed failure and interruption records is particularly concerning, as projections from one architectural family of machines to another are widely disputed. To this end, this paper introduces the Computer Failure Data Repository and issues a call for failure history data to publish in it."
}

@inproceedings{Schroeder:2009:DEW:1555349.1555372,
 author = {Schroeder, Bianca and Pinheiro, Eduardo and Weber, Wolf-Dietrich},
 title = {DRAM errors in the wild: a large-scale field study},
 booktitle = {Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems},
 series = {SIGMETRICS '09},
 year = {2009},
 isbn = {978-1-60558-511-6},
 location = {Seattle, WA, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1555349.1555372},
 doi = {10.1145/1555349.1555372},
 acmid = {1555372},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data corruption, dimm, dram, dram reliability, ecc, empirical study, hard error, large-scale systems, memory, soft error},
} 

@article{Hwang:2012:CRD:2189750.2150989,
 author = {Hwang, Andy A. and Stefanovici, Ioan A. and Schroeder, Bianca},
 title = {Cosmic rays don't strike twice: understanding the nature of DRAM errors and the implications for system design},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {March 2012},
 volume = {40},
 number = {1},
 month = mar,
 year = {2012},
 issn = {0163-5964},
 pages = {111--122},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2189750.2150989},
 doi = {10.1145/2189750.2150989},
 acmid = {2150989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DRAM errors, correctable errors, field study, reliability, uncorrectable errors},
} 

@article{10.1109/TC.2011.106,
author = {Xuejun Yang and Zhiyuan Wang and Jingling Xue and Yun Zhou},
title = {The Reliability Wall for Exascale Supercomputing},
journal ={IEEE Transactions on Computers},
volume = {61},
issn = {0018-9340},
year = {2012},
pages = {767-779},
doi = {http://doi.ieeecomputersociety.org/10.1109/TC.2011.106},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@article{fagg01harness,
  author        = "Graham Fagg and Antonin Bukovsky and Jack Dongarra",
  title         = "{Harness} and fault tolerant {MPI}",
  journal       = "Parallel Computing",
  volume        = "27",
  number        = "11",
  pages         = "1479--1495",
  year          = "2001",
  publisher     = PUBLISHER_ELSEVIER,
  issn          = "0167-8191",
  url           = "http://dx.doi.org/10.1016/S0167-8191(01)00100-4",
  absrtract     = "Initial versions of MPI were designed to work efficiently on multi-processors which had very little job control and thus static process models. Subsequently forcing them to support a dynamic process model would have affected their performance. As current HPC systems increase in size with greater potential levels of individual node failure, the need arises for new fault tolerant systems to be developed. Here we present a new implementation of MPI called fault tolerant MPI (FT-MPI) that allows the semantics and associated modes of failures to be explicitly controlled by an application via a modified MPI API. Given is an overview of the FT-MPI semantics, design, example applications, debugging tools and some performance issues. Also discussed is the experimental HARNESS core (G_HCORE) implementation that FT-MPI is built to operate upon."
}

@conference{fagg01fault,
  author        = "Graham Fagg and Antonin Bukovsky and Jack Dongarra",
  title         = "Fault-tolerant {MPI} for the {Harness} Metacomputing System",
  booktitle     = "Lecture Notes in Computer Science: Proceedings of the $1^{st}$ International Conference on Computational Science ({ICCS}) 2002, Part I",
  volume        = "2073",
  pages         = "355--366",
  month         = may # "~28-30, ",
  year          = "2001",
  address       = "San Francisco, CA, USA",
  publisher     = PUBLISHER_SPRINGER,
  issn          = "0302-9743",
  url           = "http://www.netlib.org/utk/people/JackDongarra/PAPERS/ft-harness-iccs2001.ps",
  abstract      = "Initial versions of MPI were designed to work efficiently on multi-processors which had very little job control and thus static process models. Subsequently forcing them to support a dynamic process model suitable for use on clusters or distributed systems would have reduced their performance. As current HPC collaborative applications increase in size and distribution the potential levels of node and network failures increase the need arises for new fault tolerant systems to be developed. Here we present a new implementation of MPI called FT-MPI that allows the semantics and associated modes of failures to be explicitly controlled by an application via a modified MPI API. Given is an overview of the FT-MPI semantics, design, example applications and some performance issues such as efficient group communications and complex data handling."
}

@article{hursey12:parco:hips,
	Author = {Joshua Hursey and Richard Graham},
	Doi = {10.1016/j.parco.2011.10.010},
	Issn = {0167-8191},
	Journal = {Parallel Computing},
	Number = {1-2},
	Pages = {15-25},
	Title = {Analyzing Fault Aware Collective Performance in a Process Fault Tolerant {MPI}},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167819111001414},
	Volume = {38},
	Year = {2012}
}

@InProceedings{hursey11:ft_ring,
  author =   {Joshua Hursey and Richard Graham},
  title =    {Building a Fault Tolerant {MPI} Application: A Ring Communication Example},
  booktitle =    {16th International Workshop on Dependable Parallel, Distributed and Network-Centric Systems (DPDNS) held in conjunction with the 25th {IEEE} International Parallel and Distributed Processing Symposium (IPDPS)},
  year =         2011,
  address =      {Anchorage, Alaska},
  month =        {May}
}

@InProceedings{hursey11:ft_coll,
  author =   {Joshua Hursey and Richard Graham},
  title =    {Preserving Collective Performance Across Process Failure for a Fault Tolerant {MPI}},
  booktitle =    {16th International Workshop on High-Level Parallel Programming Models and Supportive Environments (HIPS) held in conjunction with the 25th {IEEE} International Parallel and Distributed Processing Symposium (IPDPS)},
  year =         2011,
  address =      {Anchorage, Alaska},
  month =        {May}
}

@conference{fiala12detection2,
  author        = "David Fiala
                   and Frank Mueller
                   and Christian Engelmann
                   and Kurt Ferreira
                   and Ron Brightwell
                   and Rolf Riesen",
  title         = "Detection and Correction of Silent Data Corruption for
                   Large-Scale High-Performance Computing",
  booktitle     = "Proceedings of the
                   \href{http://sc12.supercomputing.org}{$25^{th}$ IEEE/ACM
                   International Conference on High Performance Computing,
                   Networking, Storage and Analysis (SC) 2012}",
  pages         = "",
  month         = nov # "~10-16, ",
  year          = "2012",
  address       = "Salt Lake City, UT, USA",
  publisher     = "\href{http://www.acm.org}{ACM Press, New York, NY, USA}",
  isbn          = "",
  doi           = "",
  url           = "",
  url2          = "",
  abstract      = "Faults have become the norm rather than the exception for
                   high-end computing on clusters with 10s/100s of thousands of
                   cores. Exacerbating this situation, some of these faults
                   remain undetected, manifesting themselves as silent errors
                   that corrupt memory while applications continue to operate
                   and report incorrect results.
                   This paper studies the potential for redundancy to both
                   detect and correct soft errors in MPI message-passing
                   applications. Our study investigates the challenges inherent
                   to detecting soft errors within MPI application while
                   providing transparent MPI redundancy. By assuming a model
                   wherein corruption in application data manifests itself by
                   producing differing MPI message data between replicas, we
                   study the best suited protocols for detecting and correcting
                   MPI data that is the result of corruption.
                   To experimentally validate our proposed detection and
                   correction protocols, we introduce RedMPI, an MPI library
                   which resides in the MPI profiling layer. RedMPI is capable
                   of both online detection and correction of soft errors that
                   occur in MPI applications without requiring any
                   modifications to the application source by utilizing either
                   double or triple redundancy.
                   Our results indicate that our most efficient consistency
                   protocol can successfully protect applications experiencing
                   even high rates of silent data corruption with runtime
                   overheads between 0\% and 30\% as compared to unprotected
                   applications without redundancy.
                   Using our fault injector within RedMPI, we observe that even
                   a single soft error can have profound effects on running
                   applications, causing a cascading pattern of corruption in
                   most cases causes that spreads to all other processes.
                   RedMPI's protection has been shown to successfully mitigate
                   the effects of soft errors while allowing applications to
                   complete with correct results even in the face of errors."
}
@conference{elliott12combining,
  author        = "James Elliott
                   and Kishor Kharbas
                   and David Fiala
                   and Frank Mueller
                   and Kurt Ferreira
                   and Christian Engelmann",
  title         = "Combining Partial Redundancy and Checkpointing for {HPC}",
  booktitle     = "Proceedings of the \href{http://icdcs-2012.org/}
                   {$32^{nd}$ International Conference on Distributed
                   Computing Systems (ICDCS) 2012}",
  pages         = "615-626",
  month         = jun # "~18-21, ",
  year          = "2012",
  address       = "Macau, SAR, China",
  publisher     = "\href{http://www.computer.org}{IEEE Computer Society, Los
                   Alamitos, CA, USA}",
  isbn          = "978-0-7695-4685-8",
  issn          = "1063-6927",
  doi           = "http://dx.doi.org/10.1109/ICDCS.2012.56",
  url           = "http://www.christian-engelmann.info/publications/elliott12combining.pdf",
  url2          = "http://www.christian-engelmann.info/publications/elliott12combining.ppt.pdf",
  abstract      = "Today's largest High Performance Computing (HPC) systems
                   exceed one Petaflops (10^15 floating point operations per
                   second) and exascale systems are projected within seven
                   years. But reliability is becoming one of the major
                   challenges faced by exascale computing. With billion-core
                   parallelism, the mean time to failure is projected to be in
                   the range of minutes or hours instead of days. Failures are
                   becoming the norm rather than the exception during execution
                   of HPC applications. Current fault tolerance techniques in
                   HPC focus on reactive ways to mitigate faults, namely via
                   checkpoint and restart (C/R). Apart from storage overheads,
                   C/R-based fault recovery comes at an additional cost in
                   terms of application performance because normal execution
                   is disrupted when checkpoints are taken. Studies have shown
                   that applications running at a large scale spend more than
                   50\% of their total time saving checkpoints, restarting and
                   redoing lost work. Redundancy is another fault tolerance
                   technique, which employs redundant processes performing the
                   same task. If a process fails, a replica of it can take over
                   its execution. Thus, redundant copies can decrease the
                   overall failure rate. The downside of redundancy is that
                   extra resources are required and there is an additional
                   overhead on communication and synchronization. This work
                   contributes a model and analyzes the benefit of C/R in
                   coordination with redundancy at different degrees to
                   minimize the total wallclock time and resources utilization
                   of HPC applications. We further conduct experiments with an
                   implementation of redundancy within the MPI layer on a
                   cluster. Our experimental results confirm the benefit of dual
                   and triple redundancy - but not for partial redundancy - and
                   show a close fit to the model. At 80,000 processes, dual
                   redundancy requires twice the number of processing resources
                   for an application but allows two jobs of 128 hours wallclock
                   time to finish within the time of just one job without
                   redundancy. For narrow ranges of processor counts, partial
                   redundancy results in the lowest time. Once the count exceeds
                   770, 000, triple redundancy has the lowest overall cost.
                   Thus, redundancy allows one to trade-off additional resource
                   requirements against wallclock time, which provides a tuning
                   knob for users to adapt to resource availabilities."
}

@conference{boehm12file,
  author        = "Swen B{\"o}hm and
                   Christian Engelmann",
  title         = "File {I/O} for {MPI} Applications in Redundant Execution
                   Scenarios",
  booktitle     = "Proceedings of the \href{http://www.pdp2012.org}{$20^{th}$
                   Euromicro International Conference on Parallel, Distributed,
                   and network-based Processing (PDP) 2012}",
  pages         = "112-119",
  month         = feb # "~15-17, ",
  year          = "2012",
  address       = "Garching, Germany",
  publisher     = "\href{http://www.computer.org}{IEEE Computer Society, Los
                   Alamitos, CA, USA}",
  isbn          = "978-0-7695-4633-9",
  issn          = "1066-6192",
  doi           = "http://dx.doi.org/10.1109/PDP.2012.22",
  url           = "http://www.christian-engelmann.info/publications/boehm12file.pdf",
  url2          = "http://www.christian-engelmann.info/publications/boehm12file.ppt.pdf",
  abstract      = "As multi-petascale and exa-scale high-performance computing
                   (HPC) systems inevitably have to deal with a number of
                   resilience challenges, such as a significant growth in
                   component count and smaller circuit sizes with lower circuit
                   voltages, redundancy may offer an acceptable level of
                   resilience that traditional fault tolerance techniques, such
                   as checkpoint/restart, do not. Although redundancy in HPC is
                   quite controversial due to the associated cost for redundant
                   components,  the constantly increasing number of
                   cores-per-processor is tilting this cost calculation toward
                   a system design where computation, such as for redundancy,
                   is much cheaper and communication, needed for
                   checkpoint/restart, is much more expensive. Recent research
                   and development activities in redundancy for Message Passing
                   Interface (MPI) applications focused on
                   availability/reliability models and replication algorithms.
                   This paper takes a first step toward solving an open research
                   problem associated with running a parallel application
                   redundantly, which is file I/O under redundancy. The
                   approach intercepts file I/O calls made by a redundant
                   application to employ coordination protocols that execute
                   file I/O operations in a redundancy-oblivious fashion when
                   accessing a node-local file system, or in a redundancy-aware
                   fashion when accessing a shared networked file system.
                   A proof-of concept prototype is presented and a number of
                   coordination protocols are described and evaluated. The
                   results show the performance impact for redundantly accessing
                   a shared networked file system, but also demonstrate the
                   capability to regain performance by utilizing MPI
                   communication between replicas and parallel file I/O."
}

@conference{engelmann11redundant,
  author        = "Christian Engelmann
                   and Swen B{\"o}hm",
  title         = "Redundant Execution of {HPC} Applications with {MR-MPI}",
  booktitle     = "Proceedings of the
                   \href{http://www.iasted.org/conferences/home-719.html}
                   {$10^{th}$ IASTED International Conference on Parallel and
                   Distributed Computing and Networks (PDCN) 2011}",
  pages         = "31--38",
  month         = feb # "~15-17, ",
  year          = "2011",
  address       = "Innsbruck, Austria",
  publisher     = "\href{http://www.actapress.com}{ACTA Press, Calgary, AB,
                   Canada}",
  isbn          = "978-0-88986-864-9",
  doi           = "http://dx.doi.org/10.2316/P.2011.719-031",
  url           = "http://www.christian-engelmann.info/publications/engelmann11redundant.pdf",
  url2          = "http://www.christian-engelmann.info/publications/engelmann11redundant.ppt.pdf",
  abstract      = "This paper presents a modular-redundant Message Passing
                   Interface (MPI) solution, MR-MPI, for transparently executing 
                   high-performance computing (HPC) applications in a redundant
                   fashion. The presented work addresses the deficiencies of
                   recovery-oriented HPC, i.e., checkpoint/restart to/from a
                   parallel file system, at extreme scale by adding the
                   redundancy approach to the HPC resilience portfolio. It
                   utilizes the MPI performance tool interface, PMPI, to
                   transparently intercept MPI calls from an application and to
                   hide all redundancy-related mechanisms. A redundantly
                   executed application runs with $r*m$ native MPI processes,
                   where $r$ is the number of MPI ranks visible to the
                   application and $m$ is the replication degree. Messages
                   between redundant nodes are replicated. Partial replication
                   for tunable resilience is supported. The performance results
                   clearly show the negative impact of the O(m^2) messages
                   between replicas. For low-level, point-to-point benchmarks,
                   the impact can be as high as the replication degree. For
                   applications, performance highly depends on the actual
                   communication types and counts. On single-core systems, the
                   overhead can be 0\% for embarrassingly parallel applications
                   independent of the employed redundancy configuration or up
                   to 70-90\% for communication-intensive applications in a
                   dual-redundant configuration. On multi-core systems, the
                   overhead can be significantly higher due to the additional
                   communication contention."
}

@conference{engelmann09case,
  author        = "Christian Engelmann
                   and Hong H. Ong
                   and Stephen L. Scott",
  title         = "The Case for Modular Redundancy in Large-Scale High
                   Performance Computing Systems",
  booktitle     = "Proceedings of the
                   \href{http://www.iasted.org/conferences/home-641.html}
                   {$8^{th}$ IASTED International Conference on Parallel and
                   Distributed Computing and Networks (PDCN) 2009}",
  pages         = "189--194",
  month         = feb # "~16-18, ",
  year          = "2009",
  address       = "Innsbruck, Austria",
  publisher     = "\href{http://www.actapress.com}{ACTA Press, Calgary, AB,
                   Canada}",
  isbn          = "978-0-88986-784-0",
  doi           = "http://www.actapress.com/Abstract.aspx?paperId=34612",
  url           = "http://www.christian-engelmann.info/publications/engelmann09case.pdf",
  url2          = "http://www.christian-engelmann.info/publications/engelmann09case.ppt.pdf",
  abstract      = "Recent investigations into resilience of large-scale
                   high-performance computing (HPC) systems showed a continuous
                   trend of decreasing reliability and availability. Newly
                   installed systems have a lower mean-time to failure (MTTF)
                   and a higher mean-time to recover (MTTR) than their
                   predecessors. Modular redundancy is being used in many
                   mission critical systems today to provide for resilience,
                   such as for aerospace and command & control systems. The
                   primary argument against modular redundancy for resilience
                   in HPC has always been that the capability of a HPC system,
                   and respective return on investment, would be significantly
                   reduced. We argue that modular redundancy can significantly
                   increase compute node availability as it removes the impact
                   of scale from single compute node MTTR. We further argue that
                   single compute nodes can be much less reliable, and therefore
                   less expensive, and still be highly available, if their
                   MTTR/MTTF ratio is maintained."
}

@conference{boehm11xsim,
  author        = "Swen B{\"o}hm and
                   Christian Engelmann",
  title         = "{xSim}: {The} Extreme-Scale Simulator",
  booktitle     = "Proceedings of the
                   \href{http://hpcs11.cisedu.info}{International Conference on
                   High Performance Computing and Simulation (HPCS) 2011}",
  pages         = "280-286",
  month         = jul # "~4-8, ",
  year          = "2011",
  address       = "Istanbul, Turkey",
  publisher     = "\href{http://www.computer.org}{IEEE Computer Society, Los
                   Alamitos, CA, USA}",
  isbn          = "978-1-61284-383-4",
  doi           = "http://dx.doi.org/10.1109/HPCSim.2011.5999835",
  url           = "http://www.christian-engelmann.info/publications/boehm11xsim.pdf",
  url2          = "http://www.christian-engelmann.info/publications/boehm11xsim.ppt.pdf",
  abstract      = "Investigating parallel application performance properties at
                   scale is becoming an important part of high-performance
                   computing (HPC) application development and deployment. The
                   Extreme-scale Simulator (xSim) is a performance investigation
                   toolkit that permits running an application in a controlled
                   environment at extreme scale without the need for a
                   respective extreme-scale HPC system. Using a lightweight
                   parallel discrete event simulation, xSim executes a parallel
                   application with a virtual wall clock time, such that
                   performance data can be extracted based on a processor model
                   and a network model. This paper presents significant
                   enhancements to the xSim toolkit prototype that provide a
                   more complete Message Passing Interface (MPI) support and
                   improve its versatility. These enhancements include full
                   virtual MPI group, communicator and collective communication
                   support, and global variables support. The new capabilities
                   are demonstrated by executing the entire NAS Parallel
                   Benchmark suite in a simulated HPC environment."
}

@conference{engelmann10facilitating,
  author        = "Christian Engelmann and
                   Frank Lauer",
  title         = "Facilitating Co-Design for Extreme-Scale Systems Through
                   Lightweight Simulation",
  booktitle     = "Proceedings of the
                   \href{http://www.cluster2010.org}{$12^{th}$ IEEE
                   International Conference on Cluster Computing (Cluster)
                   2010}: \href{http://www2.wmin.ac.uk/getovv/aacec10.html}
                   {$1^{st}$ Workshop on Application/Architecture Co-design for
                   Extreme-scale Computing (AACEC)}",
  pages         = "1-8",
  month         = sep # "~20-24, ",
  year          = "2010",
  address       = "Hersonissos, Crete, Greece",
  publisher     = "\href{http://www.computer.org}{IEEE Computer Society, Los
                   Alamitos, CA, USA}",
  isbn          = "978-1-4244-8395-2",
  doi           = "http://dx.doi.org/10.1109/CLUSTERWKSP.2010.5613113",
  url           = "http://www.christian-engelmann.info/publications/engelmann10facilitating.pdf",
  url2          = "http://www.christian-engelmann.info/publications/engelmann10facilitating.ppt.pdf",
  abstract      = "This work focuses on tools for investigating algorithm
                   performance at extreme scale with millions of concurrent
                   threads and for evaluating the impact of future architecture
                   choices to facilitate the co-design of high-performance
                   computing (HPC) architectures and applications. The approach
                   focuses on lightweight simulation of extreme-scale HPC
                   systems with the needed amount of accuracy. The prototype
                   presented in this paper is able to provide this capability
                   using a parallel discrete event simulation (PDES), such that
                   a Message Passing Interface (MPI) application can be executed
                   at extreme scale, and its performance properties can be
                   evaluated. The results of an initial prototype are
                   encouraging as a simple hello world MPI program could be
                   scaled up to 1,048,576 virtual MPI processes on a four-node
                   cluster, and the performance properties of two MPI programs
                   could be evaluated at up to 1,024 and 16,384 virtual MPI
                   processes on the same system."
}

@conference{jones11simulation,
  author        = "Ian S. Jones and
                   Christian Engelmann",
  title         = "Simulation of Large-Scale {HPC} Architectures",
  booktitle     = "Proceedings of the
                   \href{http://icpp2011.org}{$40^{th}$ International Conference
                   on Parallel Processing (ICPP) 2011}:
                   \href{http://www.psti-workshop.org} {$2^{nd}$ International
                   Workshop on Parallel Software Tools and Tool Infrastructures
                   (PSTI)}",
  pages         = "447-456",
  month         = sep # "~13-19, ",
  year          = "2011",
  address       = "Taipei, Taiwan",
  publisher     = "\href{http://www.computer.org}{IEEE Computer Society, Los
                   Alamitos, CA, USA}",
  isbn          = "978-0-7695-4511-0",
  issn          = "1530-2016",
  doi           = "http://dx.doi.org/10.1109/ICPPW.2011.44",
  url           = "http://www.christian-engelmann.info/publications/jones11simulation.pdf",
  url2          = "http://www.christian-engelmann.info/publications/jones11simulation.ppt.pdf",
  abstract      = "The Extreme-scale Simulator (xSim) is a recently developed
                   performance investigation toolkit that permits running
                   high-performance computing (HPC) applications in a controlled
                   environment with millions of concurrent execution threads. It
                   allows observing parallel application performance properties
                   in a simulated extreme-scale HPC system to further assist in
                   HPC hardware and application software co-design on the road
                   toward multi-petascale and exascale computing. This paper
                   presents a newly implemented network model for the xSim
                   performance investigation toolkit that is capable of
                   providing simulation support for a variety of HPC network
                   architectures with the appropriate trade-off between
                   simulation scalability and accuracy. The taken approach
                   focuses on a scalable distributed solution with latency and
                   bandwidth restrictions for the simulated network. Different
                   network architectures, such as star, ring, mesh, torus,
                   twisted torus and tree, as well as hierarchical combinations,
                   such as to simulate network-on-chip and network-on-node, are
                   supported. Network traffic congestion modeling is omitted to
                   gain simulation scalability by reducing simulation accuracy."
}