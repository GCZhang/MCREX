\documentclass[letterpaper,11pt]{article}
\usepackage[top=1.0in,bottom=1.0in,left=1.0in,right=1.0in]{geometry}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tmadd,tmath}
\usepackage{longtable}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{tmath,tmadd}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[mathcal]{euscript} 
\usepackage[usenames]{color}
\usepackage[
naturalnames = true, 
colorlinks = true, 
linkcolor = black,
anchorcolor = black,
citecolor = black,
menucolor = black,
urlcolor = blue
]{hyperref}

%%---------------------------------------------------------------------------%%
\author{Stuart R. Slattery, Steven P. Hamilton, and Thomas M. Evans
  \\ \href{mailto:slatterysr@ornl.gov}{\texttt{slatterysr@ornl.gov}} }

\date{\today} \title{A Parallel and Preconditioned Monte Carlo
  Synthetic-Acceleration Method for the Simplified $P_N$
  Approximation}
\begin{document}
\maketitle

%%---------------------------------------------------------------------------%%
\abstract

We present Monte Carlo Synthetic Acceleration (MCSA) as a linear
solver technique for the simplified $P_N$ ($SP_N$) discretization of
the Boltzmann neutron transport equation. Using the fully-formed
linear operators for the transport problem, we explore solutions to
the $SP_N$ equations with MCSA using light water criticality
calculations as the driving problems. Due to neutron scattering in the
light water moderator, several difficulties arise when applying MCSA
to these problems that were not observed in previous work where the
method was applied to more opaque thermal radiaton transport
systems. In addition to numerical difficulties, the level of fidelity
needed for the accurate solution of these problems creates enough
degrees of freedom to warrant domain decomposition on hundreds to
thousands of cores in order to obtain both a time-efficient solution
and enough local memory to solve the problem. To effectively solve the
$SP_N$ equations for these systems, we take two steps. First, an
algebraic preconditioning strategy is developed for MCSA to address
convergence issues. We utilize a sparse approximate inverse
preconditioner and a sparse matrix-matrix mutliplication kernel to
maintain element-level access to the preconditioned linear operator
and to improve iterative performance. Second, we have parallelized the
Monte Carlo solver by constructing a variant of fully asynchronous
domain decomposition algorithms for Monte Carlo particle transport to
enable solutions on a large number of processors. Using the
preconditioning strategy and domain decomposition algorithm, MCSA
performance is compared with production implementations of Krylov
methods and state-of-the-art algebraic preconditioners for two
eigenvalue problems modeled in three dimensions: a light water reactor
assembly and the C5G7 criticality benchmark. Our studies indicate that
the performance of MCSA when used to solve the $SP_N$ equations for
light water reactor problems in terms of both iterations to converge
and time to solution can be competitive with the preconditioned Krylov
methods when the operator spectrum is properly conditioned using our
sparse approximate inverse strategy. In addition, scalability of the
new domain decomposition algorithm is compared to traditional methods
with the scheme demonstrating good performance on thousands of
processors.

%%---------------------------------------------------------------------------%%
\section{Introduction}

Predictive modeling and simulation capability requires the combination
of high fidelity models, high performance computing hardware, and
modern algorithms. For nuclear reactor analysis, this predictive
capability can enable tighter design tolerances for improved thermal
performance and efficiency, higher fuel utilization and burnup, and
high confidence in accident scenario models. Modern high fidelity
deterministic methods for large scale problems where the solution over
the entire core of a reactor is desired are often variants on the
discrete ordinates ($S_N$) method \cite{evans_denovo:_2010} or the
method of characteristics \cite{askew_1972}. For fission reactor
neutronics simulations, the discrete ordinates method requires
potentially trillions of unknown angular flux moments to be computed
to achieve good accuracy for the responses of interest
\cite{slaybaugh_acceleration_2011} which in many cases can be
intractable without sufficient computational hardware. On the other
side of the spectrum, low-fidelity diffusion solutions are often
leveraged as an alternative to computing a solution over the entire
domain of a reactor core where diffusion theory is often valid.

In the 1960's, Gelbard developed an ad-hoc, multidimensional extension
of the simple one-dimensional planar $P_N$ equations that created a
system of coupled, diffusion-like equations known as the simplified
$P_N$ ($SP_N$) equations \cite{gelbard_1960} which incorporate
additional angular information. Further work in both aysmptotic
analysis and algorithmic development proceeded in the following
decades to serve as a theoretical basis for the method
\cite{morel_1996,brantley_simplified_2000}. Recent work has expanded
the discretization to a fully algebraic form ammenable to solution
with modern linear algebra techniques including Krylov solvers and
advanced preconditioners including multigrid methods and incomplete
factorizations \cite{hamilton_2014}. Using these new techniques, the
reduction in numerical complexity compared to current deterministic
solution methods using the $SP_N$ approximation could mean significant
savings in both compute time and memory required and permit larger
simulations to be performed on the available computing platforms using
standard tools.

Looking towards future computing platforms, as machines begin to
operate at hundreds of petaflops peak performance and beyond, trends
toward reduced energy consumption will require incredibly high levels
of concurrency to achieve the desired computation rates
\cite{kogge_using_2011}. The end result of these hardware changes is
that the large number of low-powered processors populating the machine
will be prone to both soft failures such as bit errors in floating
point operations and hard failures where the data owned by that
processor cannot be recovered
\cite{u.s._department_of_energy_resilient_2012}. Because these
failures are predicted to be common, resilient solver technologies are
a possible means of mitigating the impact of these events at the
application level. With solutions to deterministic transport problems
based on Monte Carlo techniques, such issues are potentially
alleviated by statistical arguments. In the case of soft failures,
isolated floating point errors in the Monte Carlo simulation are
absorbed within tally statistics while a hard failure is treated as a
high variance event where some portion of the Monte Carlo histories
are lost.

The goal of this work is to research and develop Monte Carlo Synthetic
Acceleration (MCSA) methods for neutron transport problems at
scale. These methods are based on using the Monte Carlo method to
directly accelerate the convergence of traditional fixed-point
iterative schemes for these problems and have the potential to improve
the resiliency of large-scale solutions to the neutron transport
problem. We aim to address both the benefits and shortcomings of MCSA
in the context of the physics of interest and desire to identify areas
in which improvements can be made to the algorithm so that the
tradeoff between algorithmic performance and the potential for
resilience at scale is not too great. In previous work
\cite{evans_monte_2014}, we applied MCSA to the thermal radiation
diffusion equation where, for several transient problems, we observed
that MCSA performed competetively with preconditioned Krylov
methods. In a continuation of these studies, we apply the method to
light water reactor problems discretized by the $SP_N$ equations. This
system has no time dependence, a large scattering component, and
assymetry in the linear operator, creating a set of problems
complementary to the transient, opaque, and symmetric systems of our
previous analysis. For problems of this type, our results indicate
that the primitive diagonal preconditioning used in our previous work
is not suitable to achieve convergence with the method. In addition,
the problems studied in this work are large enough that domain
decomposition is required in order to obtain enough memory as well as
time-efficient solutions.

To enable solutions of the $SP_N$ equations for these problems, this
work develops two new techniques. First, we define an algebraic
preconditioning strategy for MCSA that leverages an explicitly formed
preconditioner as well as a matrix-matrix product to construct the
matrix elements of the preconditioned system. As a preconditioner,
this work uses a sparse approximate inverse technique which results in
a sparse matrix with element-wise access, a necessary requirement for
implementation of the Neumann-Ulam Monte Carlo algorithm. Second, we
develop a new domain decomposition scheme for MCSA with a new fully
asynchronous domain decomposed Monte Carlo kernel for the Neumann-Ulam
algorithm. In tandem, these two developments permit solutions of the
$SP_N$ equations for large-scale light water criticality problems. We
believe both of these contributions in preconditioning and
parallelizing the Neumann-Ulam algorithm are new and unique. Although
we have motivated this work by looking forward to exascale computing,
resiliency of this solution technique in the presence of faults will
not be considered here but instead be addressed in future work.

The paper is organized as follows. First, we briefly introduce the
$SP_N$ equations for eigenvalue problems along with the discretization
used for this work. We then describe the application of the MCSA
algorithm to the solution of the equations and introduce the
criticality problems used for the numerical studies in the this
work. Next, we present our new preconditioning scheme consisting of an
approximate inverse algorithm combined with solving an approximate
Monte Carlo problem for the acceleration. We then parallelize the MCSA
algorithm and describe our new domain decomposition scheme for the
Neumann-Ulam Monte Carlo algorithm designed to enable very large
problems. We then apply the preconditioning scheme and the parallel
algorithm to the criticality problems and compare the performance of
the preconditioned MCSA method to preconditioned Krylov methods. We
then finish with conclusions and indication of future work.

%%---------------------------------------------------------------------------%%
\section{Simplified $P_N$ Approximation}

Steven or Tom....

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Synthetic Acceleration}
\label{sec:mcsa}
In this section we briefly review an adjoint variant the Monte Carlo
method of Neumann and Ulam for linear systems and Monte Carlo
Synthetic Acceleration.

\subsection{Neumann-Ulam Monte Carlo Algorithm}
An alternative approach to approximate matrix inversion is to employ
Monte Carlo methods that sample a distribution with an expectation
value equivalent to that of the inverted operator. Such methods have
been in existence for decades with the earliest reference noted here a
manuscript published in 1950 by Forsythe and Leibler
\cite{forsythe_matrix_1950}. In their outline, Forsythe and Leibler
in fact credit the creation of this technique to J. Von Neumann and
S.M. Ulam some years earlier than its publication. In 1952 Wasow
provided a more formal explanation of Von Neumann and Ulam's method
\cite{wasow_note_1952} and Hammersley and Handscomb's 1964 monograph
\cite{hammersley_monte_1964} and Spanier and Gelbard's 1969 book
\cite{spanier_monte_1969} present additional detail on this topic
using a collection of references from the 1950's and early
1960's which we use here in our discussion.

To derive the algoritm, consider a linear problem:
\begin{equation}
  \ve{A} \ve{x} = \ve{b}\:,
  \label{eq:linear_problem}
\end{equation}
where $\ve{A} \in \mathbb{R}^{N \times N}$, $\ve{x} \in \mathbb{R}^N$,
and $\ve{b} \in \mathbb{R}^N$. We define the linear system adjoint to
Eq~(\ref{eq:linear_problem}):
\begin{equation}
  \ve{A}^T \ve{y} = \ve{d}\:,
  \label{eq:adjoint_linear_problem}
\end{equation}
where $\ve{y}$ and $\ve{d}$ are the adjoint solution and source
vectors respectively and $\ve{A}^T$ is the adjoint operator for
$\ve{A} \in \mathbb{R}^{N \times N}$ which we then split to get:
\begin{equation}
  \ve{y} = \ve{H}^T \ve{y} + \ve{d}\:,
  \label{eq:adjoint_split_system}
\end{equation}
where $\ve{H}^T = \ve{I} - \ve{A}^T$ is the adjoint \textit{iteration
  matrix}.  We then solve for $\ve{y}$ by writing
Eq~(\ref{eq:adjoint_split_system}) as:
\begin{equation}
  \ve{y} = (\ve{I} - \ve{H}^T)^{-1} \ve{d}\:,
  \label{eq:adjoint_split_system_2}
\end{equation}
which in turn yields the Neumann series using the adjoint operators:
\begin{equation}
  \ve{y} = \sum_{k=0}^{\infty} (\ve{H}^T)^k\ve{d}\:.
  \label{eq:adjoint_neumann_series}
\end{equation}

The Monte Carlo algorithm is generated by expanding
Eq~(\ref{eq:adjoint_neumann_series}) into individual terms:
\begin{equation}
  y_i = \sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
  \sum_{i_k}^{N}h^T_{i,i_1} h^T_{i_1,i_2}\ldots h^T_{i_{k-1},i_k}
  d_{i_k}\:,
  \label{eq:adjoint_neumann_solution}
\end{equation}
which can interpreted as a series of transitions between states of the 
adjoint iteration matrix,
\begin{equation}
 \nu = i \rightarrow i_1 \rightarrow \cdots \rightarrow i_{k-1}
 \rightarrow i_{k}\:,
  \label{eq:mc_walk_permutation}
\end{equation}
where $\nu$ is interpreted as a particular sequence instance. If we
consider $\nu$ to be a sequence created by a Markov chain, we can
build probabilities, weights, and a stochastic estimator to play a
Monte Carlo game whos expectation value is the solution to
Eq~(\ref{eq:linear_problem}). To do this, in
Eqs~(\ref{eq:linear_problem}) and (\ref{eq:adjoint_linear_problem}) we
have 3 unknowns: $\ve{x}$, $\ve{y}$ and $\ve{d}$. To eliminate
$\ve{y}$ and $\ve{d}$ and give a system to solve for $\ve{x}$ we
require two constraints. By defining the following adjoint inner
product equivalence \cite{spanier_monte_1969}:
\begin{equation}
  \langle \ve{A}^T \ve{y}, \ve{x} \rangle = \langle \ve{y}, \ve{A}
  \ve{x} \rangle\:.
  \label{eq:adjoint_operator_product}
\end{equation}
the first constraint follows as:
\begin{equation}
  \langle \ve{x}, \ve{d} \rangle = \langle \ve{y}, \ve{b} \rangle\:.
  \label{eq:adjoint_vector_relation}
\end{equation}
As a second constraint we select:
\begin{equation}
  \ve{d} = \boldsymbol{\delta}_j\:,
  \label{eq:adjoint_second_constraint}
\end{equation}
where $\boldsymbol{\delta}_j$ is one of a set of vectors in which the
$j^{th}$ component is the Kronecker delta function $\delta_{i,j}$. If
we apply Eq~(\ref{eq:adjoint_second_constraint}) to our first
constraint Eq~(\ref{eq:adjoint_vector_relation}), we get the following
convenient outcome:
\begin{equation}
  \langle \ve{y}, \ve{b} \rangle = \langle \ve{x},
  \boldsymbol{\delta}_j \rangle = x_j \:,
  \label{eq:inner_product_constraint}
\end{equation}
meaning that if we compute the inner product of the original
right-hand-side of the linear system and the adjoint solution using a
delta function source, we recover one component of the original
solution.

We can build probabilities and weights by thinking in terms of
radiation transport. This adjoint variation of the Neumann-Ulam
algorithm is equivalent to a traditional forward method where the
initial state, $i_0$, of the random walk is determined by sampling
$\ve{b}$ with probabilities:
\begin{equation}
  P_{(i_0=i)}(\nu) = \frac{|b_i|}{||\ve{b}||_1}\:,
  \label{eq:adjoint_source_probability}
\end{equation}
with a random walk starting weight of:
\begin{equation}
  W_0 = ||\ve{b}||_1 \frac{b_i}{|b_i|}\:,
  \label{eq:adjoint_starting_weight}
\end{equation}
which gives the additional useful relation:
\begin{equation}
  b_{i_0} = W_0 P_{(i_0=i)}\:.
  \label{eq:adjoint_source_definition}
\end{equation}
We build weights and probabilities for the random walk using the
adjoint \textit{Neumann-Ulam decomposition} of $\ve{H}$:
\begin{equation}
  \ve{H}^{T} = \ve{P} \circ \ve{W}\:,
  \label{eq:adjoint_neumann_ulam}
\end{equation}
where $P$ is a row-stochastic matrix with each row being a probability
distribution function and $W$ a corresponding weight matrix.  The
probability matrix is defined element-wise as:
\begin{equation}
  p_{ij} = \frac{|h^T_{ij}|}{\sum_j |h^T_{ij}|}\:,
  \label{eq:adjoint_probability}
\end{equation}
such that we expect to select a new state, $j$, from the current state
in the random walk, $i$, by sampling from the distribution function in
row $i$ of the proability matrix. Per
Eq~(\ref{eq:adjoint_neumann_ulam}), the transition weight from state
$i$ to state $j$ is then defined as:
\begin{equation}
  w_{ij} = \frac{h^T_{ij}}{p_{ij}}\:.
  \label{eq:adjoint_weight}
\end{equation}

Next we define an expectation value for the random walks. The
contribution to the solution in state $j$ from a particular random
walk permutation of length $k$ is given by the \textit{collision
  estimator}:
\begin{equation}
  X_{j}(\nu) = \sum_{m=0}^k W_{m} \delta_{i_m,j}\:,
  \label{eq:adjoint_permutation_contribution}
\end{equation}
where the Kronecker delta, defined as the right-hand-side of the
adjoint linear problem in Eq~(\ref{eq:adjoint_second_constraint}),
indicates that the tally contributes only in the current state, $i_m$,
of the random walk. The expectation value using all random walk
permutations is then:
\begin{equation}
  E\{X_j\} = \sum_{\nu} P_{\nu} X_{j}(\nu)\:,
  \label{eq:adjoint_expectation_value}
\end{equation}
with $P_{\nu}$ the probability of a given random walk, $\nu$.  If
Eq~(\ref{eq:adjoint_expectation_value}) is expanded utilizing
Eq~(\ref{eq:adjoint_source_definition}) to insert the source term, we
directly recover a component of the solution to the linear problem:
\begin{equation}
  \begin{split}
    E\{X_j\} &=\sum_{k=0}^{\infty}\sum_{i_1}^{N}\sum_{i_2}^{N}\ldots
    \sum_{i_k}^{N} b_{i_0} p_{i_0,i_1}p_{i_1,i_2}\ldots
    p_{i_{k-1},i_k} w_{i_0,i_1}w_{i_1,i_2}\ldots
    w_{i_{k-1},i_k} \delta_{i_k,j} \\ &= x_{j}\:,
  \end{split}
  \label{eq:adjoint_expectation_expansion}
\end{equation}
therefore, also providing an unbiased Monte Carlo estimate of the
solution. It should be noted here that
Eq~(\ref{eq:adjoint_expectation_expansion}) only computes a single
component of our desired solution vector when really what we desire is
the entire solution vector. In an adjoint Monte Carlo simulation using
this estimator, the $w_{ij}$ elements that are added into the tally
for each state are only selected if/when the random walk currently
resides in that state. Much like a mesh tally in a particle transport
simulation, we have $N$ simultaneous tallies for $\ve{A} \in
\mathbb{R}^{N \times N}$ that will yield the entire solution vector.

Convergence of the Monte Carlo method, previously assumed by many to
be entirely bound by the spectral radius of $\ve{H}^T$
(e.g. \cite{spanier_monte_1969}), was recently identified as a only a
subset of the convergence criteria. In particular, Ji, Mascagni, and
Li demonstrate in \cite{ji_2013} that a necessary and sufficient
condition for convergence of this algorithm is 
\begin{equation}
  \rho(\ve{H^*}) < 1
  \label{eq:necessary_condition}
\end{equation} 
with $\ve{H^*}$ defined element-wise as:
\begin{equation}
  h^*_{ij} = \frac{(h^T_{ij})^2}{p_{ij}}\:,
  \label{eq:condition_values}
\end{equation}
or for our choice of probabilities via
Eq~(\ref{eq:adjoint_neumann_ulam})
\begin{equation}
  h^*_{ij} = h_{ij} w_{ij}\:,
  \label{eq:h_star}
\end{equation}
which is indicative that both the structure of $\ve{H}^T$ and the
selection of transition probabilities affect the convergence of the
algorithm.

\subsection{Monte Carlo Synthetic Acceleration}
Halton presented a residual Monte Carlo scheme that dramatically
improved performance by constructing an iterative refinement scheme
\cite{halton_sequential_1962}. Using the ideas of Halton, Evans and
Mosher recently developed a Monte Carlo solution method that was not
prohibited severely by the quality of the initial guess for the system
\cite{evans_monte_2009} and later applied it more rigorously as a
solution mechanism for the transient radiation diffusion equation
\cite{evans_monte_2014}. Their approach was instead to use residual
Monte Carlo as a synthetic acceleration for a stationary method. To
derive this method, we begin by splitting the operator in
Eq~(\ref{eq:linear_problem})
\begin{equation}
  \ve{x} = (\ve{I} - \ve{A})\ve{x} + \ve{b}\:.
  \label{eq:linear_split}
\end{equation}
With this we can then define the stationary method
\textit{Richardson's iteration} as:
\begin{equation}
  \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}\:,
  \label{eq:richardsons_iteration}
\end{equation}
which will converge if $\rho(\ve{I} - \ve{A}) < 1$. We then define the
solution error at the $k^{th}$ iterate relative to the true solution:
\begin{equation}
  \delta \ve{x}^k = \ve{x} - \ve{x}^k\:.
  \label{eq:mcsa_error}
\end{equation}
Subtracting Eq~(\ref{eq:richardsons_iteration}) from
Eq~(\ref{eq:linear_split}) we get:
\begin{equation}
  \delta \ve{x}^{k+1} = (\ve{I} - \ve{A})\delta \ve{x}^k\:.
  \label{eq:mcsa_setup_1}
\end{equation}
Subtracting from this $(\ve{I} - \ve{A})\delta \ve{x}^{k+1}$ yields:
\begin{equation}
  \begin{split}
    \ve{A}\delta \ve{x}^{k+1} &= (\ve{I} -
    \ve{A})(\ve{x}^{k+1}-\ve{x}^{k}) \\ &= \ve{r}^{k+1}\:.
    \label{eq:mcsa_setup_2}
  \end{split}
\end{equation}
Using this, we define the following scheme that will converge in one
iteration if $\ve{A}$ is inverted exactly:
\begin{subequations}
  \begin{gather}
    \ve{x}^{k+1} = (\ve{I} - \ve{A})\ve{x}^k + \ve{b}\:,\\
    \ve{A} \delta \ve{x}^{k+1} = \ve{r}^{k+1}\:,\\
    \ve{x} = \ve{x}^{k+1} + \delta \ve{x}^{k+1}\:.
  \end{gather}
  \label{eq:mcsa_setup_3}
\end{subequations}
However, $\ve{A}$ is only approximately inverted by our numerical
methods and therefore we instead pose an iterative scheme in which the
Monte Carlo solvers are used to invert the operator. The fixed-point
\textit{Monte Carlo Synthetic Acceleration} (MCSA) method is defined
as:
\begin{subequations}
  \begin{gather}
    \ve{x}^{k+1/2} = \ve{x}^k + \ve{r}^k\:,\\
    \ve{r}^{k+1/2} = \ve{b} - \ve{A}\ve{x}^{k+1/2}\:,\\
    \label{eq:mc_mcsa_solve}
    \ve{A}\delta\ve{x}^{k+1/2} = \ve{r}^{k+1/2}\:,\\
    \ve{x}^{k+1} = \ve{x}^{k+1/2} + \delta \ve{x}^{k+1/2}\:,\\
    \ve{r}^{k+1} = \ve{b} - \ve{A}\ve{x}^{k+1}\:,
  \end{gather}
  \label{eq:mcsa}
\end{subequations}
where a Neumann-Ulam Monte Carlo method is used to approximately solve
Eq~(\ref{eq:mc_mcsa_solve}).

Using Monte Carlo in this way achieves the same effect as Halton's
method, decoupling the convergence rate of the Monte Carlo from the
overall convergence rate of the method. Here, the approximate Monte
Carlo solution is not driven to a particular convergence as it merely
supplies a correction for the initial guess generated by Richardson's
iteration. Rather, only a set number of histories are required using
the Neumann-Ulam method to generate the correction. In addition, the
fact that the scheme in Eq~(\ref{eq:mcsa_setup_3}) will converge in
one iteration if $\ve{A}$ is inverted exactly means that as the number
of random walks in a given iteration are increased, stochastic error
in $\delta \ve{x}$ is reduced towards zero and the number of
iterations required for MCSA to converge should decrease accordingly,
thus accelerating the solution.

\subsection{Arnoldi Iteration with MCSA}

Steven.....

%%---------------------------------------------------------------------------%%
\section{Criticality Model Problems}

Steven or Tom....

\subsection{Light Water Reactor Assembly}

Steven or Tom....

\subsection{C5G7}

Steven or Tom....

%%---------------------------------------------------------------------------%%
\section{Preconditioning Algorithm}
In this section we present our preconditioning algorithm.

\subsection{Preconditioned MCSA}
\label{subsec:preconditioning}
In most cases, at least a minimal amount of preconditioning of the
linear system will be required in order to satisfy the necessary
convergence condition given in Eq~(\ref{eq:necessary_condition}) for
the Neumann-Ulam algorithm. Preconditioning serves as a means of
achieving this by altering the eigenvalue spectrum of the adjoint
iteration matrix and subsequently $\ve{H}^*$. It is possible to use
general left and right preconditioning with MCSA by carefully
considering the underlying Monte Carlo problem that will be solved
with the Neumann-Ulam algorithm. We consider a left preconditioner,
$\ve{M_L}$, and a right preconditioner, $\ve{M_R}$. The left/right
preconditioned linear problem is then:
\begin{equation}
  \ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}\ve{M}_R\ve{x} = \ve{M}_L^{-1}\ve{b}\:.
  \label{eq:left_right_linear_problem}
\end{equation}
To handle the right preconditioning, the system is written with a
substitution of variables:
\begin{equation}
  \ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}\ve{u} = \ve{M}_L^{-1}\ve{b}\:,
  \label{eq:left_right_subs_problem}
\end{equation}
with
\begin{equation}
  \ve{x} = \ve{M}_R^{-1}\ve{u}\:.
  \label{eq:left_right_recover}
\end{equation}
To apply this preconditioning to MCSA, we solve for the substituted
variable $\ve{u}$ during the iteration sequence:
\begin{subequations}
  \begin{gather}
    \ve{u}^{k+1/2} = \ve{u}^k + \ve{M}_L^{-1}\ve{r}^k\:,\\
    \ve{r}^{k+1/2} = \ve{b}-\ve{A}\ve{M}_R^{-1}\ve{u}^{k+1/2}\:,\\ 
    \ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}\delta\ve{u}^{k+1/2} = \ve{M}_L^{-1}\ve{r}^{k+1/2}\:,\\ 
    \ve{u}^{k+1} = \ve{u}^{k+1/2} + \delta \ve{u}^{k+1/2}\:,\\
    \ve{r}^{k+1} = \ve{b}-\ve{A}\ve{M}_R^{-1}\ve{u}^{k+1}\:,
  \end{gather}
  \label{eq:left_right_mcsa}
\end{subequations}
and then recover the original solution vector with
Eq~(\ref{eq:left_right_recover}). For the Monte Carlo problem, we
isolate the generation of the correction:
\begin{equation}
  \ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}\delta\ve{u}^{k+1/2} =
  \ve{M}_L^{-1}\ve{r}^{k+1/2}\:,
  \label{eq:left_right_correction}
\end{equation}
and note that the preconditioned residual of the substituted variable
is now serving as the source and the new adjoint iteration matrix is:
\begin{equation}
  \ve{H}^T = \big(\ve{I} - \ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}\big)^T\:.
  \label{eq:left_right_iteration_matrix}
\end{equation}
As we require element-wise access to the iteration matrix in order to
construct probabilities and weights from the Neumann-Ulam
decomposition for the Monte Carlo procedure, the composite operator,
$\ve{M}_L^{-1}\ve{A}\ve{M}_R^{-1}$, must be formed via matrix-matrix
multiplication. Provided that $\ve{M}_L^{-1}$, $\ve{A}$, and
$\ve{M}_R^{-1}$ are relatively sparse, then we can readily leverage
sparse matrix-matrix multiplication algorithms that are in wide use in
scalable algebraic multigrid implementations.

Several possible shortcomings of this preconditioning approach are
readily observed. First, the matrix-matrix multiplication operation
for sparse, parallel distributed matrices is significantly more
expensive than a matrix-vector multiplication operation. Second, each
preconditioner must be explicitly inverted, an operation in itself
that may be expensive and which prohibits the use of any
preconditioners which provide no mechanism to extract their
inverse. Third, for many modern preconditioning methods, this
inversion may yield dense matrices, destroying sparsity and further
impeding the performance of a matrix-matrix multiplication
operation. It is also interesting to note that the Monte Carlo problem
in the general left/right preconditioned scheme given by
Eq~(\ref{eq:left_right_correction}) is not fully left/right
preconditioned (meaning that we do not recover $\ve{x}$), but instead
is part of a sequence for finding the substituted variable,
$\ve{u}$. We do, however, gain the benefits of this general
preconditioning by building the iteration matrix in
Eq~(\ref{eq:left_right_iteration_matrix}) from the fully
preconditioned linear operator.

\subsection{Sparse Approximate Inverse Preconditioning}

\subsection{Approximate Monte Carlo Problem}

%%---------------------------------------------------------------------------%%
\section{Parallel Algorithm}
\label{sec:parallel_mcsa}
To date, parallel Neumann-Ulam methods have been limited to full
domain replication with parallelism exploited through individual
histories \cite{alexandrov_efficient_1998} and in this work we will
exploit particle transport algorithms to alleviate this. In this
section we briefly review particle transport methods for domain
decomposed Monte Carlo. We then devise a parallel algorithm for the
Neumann-Ulam method based on the multiple-set overlapping-domain
decomposition algorithm and a parallel algorithm for the MCSA
iteration that leverages the parallel Monte Carlo algorithm and
general parallel matrix-vector operations. We then perform parallel
scaling studies to test its performance on a leadership class machine
and compare this performance to the production Krylov methods.

\subsection{Domain Decomposed Neumann-Ulam Algorithm}
\label{subsec:asynchronous_algorithm}
In the context of radiation transport, in 2009 Brunner and colleagues
provided a fully asynchronous domain decomposed parallel algorithm as
implemented in production implicit Monte Carlo codes
\cite{brunner_efficient_2009}. We will adapt their algorithm and
directly apply it to construct a parallel formulation of the
Neumann-Ulam method. Direct analogs can be derived from their works by
noting that the primary difference between solving a linear transport
system with Monte Carlo methods and traditional fixed source Monte
Carlo transport problems is the content of the Markov chains that are
generated.

%%---------------------------------------------------------------------------%%
\section{Results}

\subsection{Light Water Reactor Fuel Assemblies}

\subsection{C5G7 Benchmark}

%%---------------------------------------------------------------------------%%
\section{Conclusion}
\label{sec:conclusion}

In this paper, the Monte Carlo Synthetic Acceleration method has been
applied to the $SP_N$ discretization of the neutron transport equation
and explored in the context of a difficult nuclear fuel assembly
criticality problem. We observed that MCSA can in fact solve the
asymmetric system generated by the $SP_N$ equations. In contrast to
the thermal radiation diffusion problem addressed in our previous
work, light water reactor problems are difficult to solve with MCSA as
they have large spectral radii due to the neutron scattering in the
moderator. Because of this, simple Jacobi-based preconditioning
reduces the spectral radius of the $SP_N$ system below unity, however,
this alone was discovered not to be sufficient criteria alone for MCSA
convergence. As the spectral radius approaches unity, MCSA was
demonstrated to break down with more stochastic histories required for
convergence and more CPU time required per history. To alleviate these
effects, a left-right preconditioned form of the MCSA method was
developed and applied to the $SP_N$ equations to obtain
convergence. For the nuclear fuel assembly problem, MCSA was observed
to converge in fewer iterations per eigenvalue iteration than GMRES
for the fuel assembly criticality problem and more than Bi-CGStab
using the same preconditioning. However, the explicit preconditioning
strategy required to overcome the MCSA spectral radius restriction
creates run times $O(100)$ larger than the production Krylov methods
for the fuel assembly criticality problem, indicating that significant
research in the area of Monte Carlo preconditioning implementations is
required in order for the method to be competetive in terms of time to
solution.

%%---------------------------------------------------------------------------%%
\pagebreak
\bibliographystyle{ieeetr} 
\bibliography{references}
\end{document}


