\documentclass[note]{TechNote}
\usepackage[centertags]{amsmath}
\usepackage{amssymb,amsthm}
\usepackage[mathcal]{euscript}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{c++}
\usepackage{tmadd,tmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}

%%---------------------------------------------------------------------------%%
\begin{document}

\refno{CSMD-00-000}
\subject{A Multilevel Monte Carlo Solver for Linear Systems}

\TIname{Mathematics and Computer Science Division}
\groupname{Computational Engineering and Energy Sciences Group}
\from{Stuart R. Slattery}
\date{\today}
\audience{
  \email{Stuart Slattery}{slatterysr@ornl.gov} \\
  \email{Tom Evans}{evanstm@ornl.gov} \\
  \email{Steven Hamilton}{hamiltonsp@ornl.gov}
}

%%---------------------------------------------------------------------------%%
\opening

\begin{abstract}
  Monte Carlo solvers for linear systems have been demonstrated to
  perform poorly for strongly elliptic problems. This poor performance
  is primarily due to the fact that both a large number of samples are
  required to obtain a good statistical error and that these samples
  require a large amount of time to compute. As a mechanism to reduce
  the computational complexity for such problems and thereby improve
  the figure of merit of the calculation, multilevel Monte Carlo has
  been introduced in finance problems, the solution of stochastic
  partial differential equations, and other algorithms that leverage
  Markov chain Monte Carlo. We adapt these ideas to form a multilevel
  Monte Carlo solver for linear systems which, in the form presented
  here, is effectively a stochastic realization of a multigrid
  solver. Numerical studies indicate that the new multilevel method
  can reduce the time required to achieve a certain statistical error
  in the solution by at least two orders of magnitude, thereby
  dramatically reducing the computational complexity of the
  problem. Furthermore, the general formulation and results presented
  here indicate that this methodology may be used with both geometric
  and algebraic formulations of the Galerkin mutlgrid method.
\end{abstract}

%%---------------------------------------------------------------------------%%
\section{Introduction}
\label{sec:introduction}
Monte Carlo solvers for linear systems have been in existence for
decades as a stochastic alternative to iterative methods
\cite{forsythe_matrix_1950,wasow_note_1952,halton_sequential_1962,hammersley_monte_1964,spanier_monte_1969}. However,
these methods have failed to gain popularity both in the mathematics
and applications community partly due to their slow convergence bound
by the central limit theorem. Recent work has indicated that when used
as an acceleration in the Monte Carlo Synthetic Acceleration (MCSA)
method, exponential convergence rates may be achieved that are
competitive with contemporary iterative methods
\cite{evans_residual_2003,evans_monte_2009,evans_monte_2012,slattery_phd_2013}.
However, in this recent work it was discovered that for physics
problems that are largely elliptic (e.g. neutron transport in a light
water reactor), convergence of the Monte Carlo method is extremely
slow and prohibitive for the solution of larger systems. One avenue to
improve the time to solution for these calculations and to enable the
solution of more difficult problems is to study preconditioning
strategies as in \cite{slattery_phd_2013}. Another approach is to
instead focus on improving the time complexity of the Monte Carlo
sequence independent of the condition number of the linear problem.

Recent work in Monte Carlo methods for problems in finance and
stochastic partial differential equations has indicated that the
computationally complexity of the problem can be dramatically reduced
by incorporating multigrid concepts into the solution scheme
\cite{heinrich_2001,giles_2008,cliffe_2011}. In this work, we adapt
those ideas and apply them to the Monte Carlo problem for linear
systems as a means of reducing the computational complexity of the
algorithm. To begin, we first introduce the elliptic model problem for
our numerical experiments and compare the spectral behavior of the
Monte Carlo method to traditional iterative smoothers that would be
used with the multigrid method. Next, we present the multilevel Monte
Carlo method for linear systems using a general algebraic
formulation. Finally, we present results using the model problem the
demonstrate the superiority of the multilevel method in terms of time
to solution using both geometric and algebraic multigrid
representations of the linear problem.

%%---------------------------------------------------------------------------%%
\section{Monte Carlo Solver Fourier Analysis}
\label{sec:fourier_analysis}
We would first like to analyze the behavior of Monte Carlo solvers in
the context of error modes for a given model problem with the
numerical analysis presented here closely following that presented in
\cite{briggs_multigrid}. For this analysis, we will use the following
one-dimensional, homogeneous model problem:
\begin{equation}
  \nabla^2 x = 0\:.
  \label{eq:model_problem}
\end{equation}
We discretize the problem into $N$ discrete points
where now $\ve{x} \in \mathbb{R}^N$ with boundary conditions:
\begin{equation}
  \ve{x}_1 = 0,\ \ \ve{x}_N = 0\:.
  \label{eq:boundary_conditions}
\end{equation}
The Laplacian is discretized using a standard second-order finite
difference with a grid spacing of one:
\begin{equation}
  (\nabla \ve{u})_i = \ve{u}_{i-1} - 2 \ve{u}_{i} + \ve{u}_{i+1}\:,
  \label{eq:discrete_laplacian}
\end{equation}
which then gives the following linear problem:
\begin{equation}
  \ve{A}\ve{x} = \ve{0}\:.
  \label{eq:linear_problem}
\end{equation}
To bound the spectral radius of the problem we will use a Jacobi
preconditioner:
\begin{equation}
  \ve{M} = diag(\ve{A})\:,
  \label{eq:preconditioner}
\end{equation}
such that we instead solve the following linear problem:
\begin{equation}
  \ve{M}^{-1}\ve{A}\ve{x} = \ve{0}\:.
  \label{eq:precond_problem}
\end{equation}
To elucidate the effect of a given solution technique on a given error
mode in the problem, we can assign an initial guess of $\ve{x}^0$ to
be a chosen Fourier mode:
\begin{equation}
  \ve{x}^0_i = \sin\Bigg( \frac{ik\pi}{N} \Bigg)\:,
  \label{eq:fourier_mode}
\end{equation}
where $\ve{x}^0_i$ is the $i^{th}$ component of the initial guess and
k is the wave number of the chosen Fourier mode. 

Monte Carlo solvers are effectively a stochastic realization of
Richardson's iteration and therefore we will first look at the
performance of Richardson's iteration as a smoother:
\begin{equation}
  \ve{x}^{k+1} = (\ve{I}-\ve{M}^{-1}\ve{A})\ve{x}^k\:,
\end{equation}
where $k$ is the iteration index and which is equivalently a Jacobi
iteration when Jacobi preconditioning is
used. Figure~\ref{fig:richardson} gives infinity norm of the error in
the solution vector\footnote{The solution to the homogeneous problem is
  zero and therefore $||\ve{e}||_{\infty} = ||\ve{x}||_{\infty}$.} as
a function of iteration for wave numbers of 1, 5, and 10 on a grid
with $J = 100$.
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=4in]{richardson.png}
  \end{center}
  \caption{\textbf{Convergence of Richardson's iteration as a function
      of iteration for a grid of size $J = 100$.} \textit{Richardson's
      iteration performs better for larger wave numbers and therefore
      more oscillatory modes.}}
  \label{fig:richardson}
\end{figure}
Immediately we note that the larger the wave number the better
Richardson's iteration performs. Per the spectral analysis in
\cite{briggs_multigrid}, this iteration sequence performs better for
higher wave numbers as the eigenvalue spectrum spans a smaller space
then less oscillatory modes. This behavior motivates the multigrid
approach where moving a smooth mode to a coarser grid makes that mode
appear more oscillatory relative to that coarser grid, thus improving
convergence for that particular mode.

Next we perform the same calculations using the adjoint Monte Carlo
solver presented in \cite{evans_monte_2012}. Before doing this
however, we must first modify the linear problem as the Monte Carlo
solver is a direct method and therefore a homogeneous problem with a
right hand side of $\ve{0}$ will yield no samples. Instead, we will
solve the residual problem:
\begin{equation}
  \ve{A}\ve{d} = \ve{r}\:,
  \label{eq:residual_problem}
\end{equation}
where the residual of the homogeneous problem is:
\begin{equation}
  \ve{r} = -\ve{A}\ve{x}^0\:,
  \label{eq:homogenous_residual}
\end{equation}
and the solution is computed as:
\begin{equation}
  \ve{x} = \ve{x}^0 + \ve{d}\:.
  \label{eq:residual_solution}
\end{equation}
Forming the problem in this way lets us directly apply the adjoint
Monte Carlo method to the homogeneous problem and then apply the
effective correction, $\ve{d}$, to the initial guess to give the
solution. This approach is also equivalent to performing a single
iteration of Halton's method \cite{halton_sequential_1962}. Using this
formulation we can again solve the model problem with wave numbers of
1, 5 and 10 on a grid of size $J = 100$ but this time we vary the
number of histories used to compute the solution instead of the number
of iterations. Figure~\ref{fig:adjoint_mc} gives the results of these
calculations.
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=4in]{adjoint_mc.png}
  \end{center}
  \caption{\textbf{Convergence of the adjoint Monte Carlo method as a
      function of sampled histories for a grid of size $J = 100$.}
    \textit{Adjoint Monte Carlo performs better for smaller wave
      numbers and therefore smoother modes.}}
  \label{fig:adjoint_mc}
\end{figure}
Surprisingly, the Monte Carlo method performs better for smooth modes
than more oscillatory modes\footnote{This behavior was also observed
  for the forward Monte Carlo method presented in
  \cite{evans_monte_2012}.}. The results of these calculations are
counterintuitive given the fact that the Monte Carlo solver is
effectively a stochastic realization of Richardson's iteration and
therefore one should expect the same spectral behavior from the
results.

Looking at the timing results in Table~\ref{tab:mc_timing}, we see
that the behavior of the Monte Carlo solver is in fact consistent with
these expectations.
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cc}\hline\hline
      \multicolumn{1}{c}{\textbf{Wave Number}} & 
      \multicolumn{1}{c}{\textbf{Time per History (s)}} \\
      \hline
      1 & 1 \\
      5 & 0.85 \\
      10 & 0.83 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Normalized average CPU time per history.}}
  \label{tab:mc_timing}
\end{table}
Timing results show that the average time required to compute an
entire history in the Monte Carlo solver decreases as a function of
wave number. We showed analytically in \cite{slattery_2013} that the
length of the random walk is equivalent to the number of Richardson
iterations that would be required to achieve a given convergence
criteria. In Figure~\ref{fig:richardson}, we see that fewer iterations
are required to converge larger wave numbers and therefore we should
also expect shorter random walks in the Monte Carlo solver and
therefore a faster time to solution as observed in
Table~\ref{tab:mc_timing}. However, this does not indicate why the
Monte Carlo solvers perform better for smoother modes rather than
oscillatory modes. Fortunately, the explanation for this behavior is
simple. Consider the plots for the Fourier modes with $k = 1$ and $k =
10$ given in Figure~\ref{fig:fourier_modes}.
\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{mode_1.pdf}
    \caption{\textbf{Fourier mode with $k = 1$.}}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{mode_10.pdf}
    \caption{\textbf{Fourier mode with $k = 10$.}}
  \end{subfigure}
  \caption{\textit{The more oscillatory the mode the finer the
      structure of the error. Finer structures require more samples to
      resolve them.}}
  \label{fig:fourier_modes}
\end{figure}
The higher the wave number is the more oscillatory the mode
appears. To resolve these fine structures in the error we therefore
naturally require more Monte Carlo samples to distinguish them from
one another. In other words, statistical noise in the solution must be
reduced by more samples in order to resolve fine structures in the
error that would otherwise be buried in the noise.

Based on this, it may not seem likely that applying Monte Carlo in a
multigrid context will be useful considering that error modes appear
more oscillatory as the grid is coarsened. This would indicate that in
order to achieve the same amount of error on a coarse grid
representation of the problem, one would be required to compute many
more samples to resolve the resulting finer structures. However, we
may gain a computational advantage for two reasons. First, consider
the timing results in Table~\ref{tab:mc_timing}. As the error
structure becomes more oscillatory, the time it takes to compute a
sample decreases. Second, if we coarsen the grid the time it takes to
compute a sample will also decrease due to the decreased problem
size. Therefore, it is plausible that we might observe an improvement
in run times for the Monte Carlo problem by using a multigrid
approach.

\clearpage

%%---------------------------------------------------------------------------%%
\section{Multilevel Monte Carlo Algorithm}
\label{sec:algorithm}
A potential improvement in computational complexity motivates applying
techniques in multilevel Markov chain Monte Carlo recently developed
by Heinrich \cite{heinrich_2001} and Giles \cite{giles_2008}. We start
first with the standard Monte Carlo estimator for the solution vector:
\begin{equation}
  \hat{\ve{x}} = \frac{1}{N} \sum_{m=1}^N x^m\:,
  \label{eq:standard_estimator}
\end{equation}
where $x^m$ is the $m^{th}$ observation. We next consider representing
the problem in multiple levels designated by $l$ with each one finer
than the next such that there are $L$ total levels and $l=L$ is the
coarsest level. We can combine expectation values of the estimates
from each of these levels using the linearity of expectation such
that:
\begin{equation}
  E(\ve{x}_{0}) = E(\ve{x}_{L}) + E(\ve{x}_{L-1} - \ve{x}_{L}) +
  E(\ve{x}_{L-2} - \ve{x}_{L-1}) + \dots + E(\ve{x}_{0} -
  \ve{x}_{1})\:,
  \label{eq:linearity_of_expectation}
\end{equation}
where $E(\ve{x}_{L})$ is the expectation value of the solution vector
at the coarsest level and $E(\ve{x}_{0})$ the expectation at the
finest level. If we rewrite this summation as:
\begin{equation}
  E(\ve{x}_{0}) = E(\ve{x}_{L}) + \sum_{l=1}^L E(\ve{x}_{l} -
  \ve{x}_{l+1})\:,
  \label{eq:linearity_of_expectation_sum}
\end{equation}
we can then build a new \textit{multilevel estimator} for the $l^{th}$
level as:
\begin{equation}
    \hat{\ve{y}}_{l} = \frac{1}{N_l} \sum_{m=1}^{N_l} (x^m_{l} -
    x^m_{l+1})\:,
    \label{eq:level_estimate}
\end{equation}
where $N_l$ are the number of samples computed at the level and
$x^m_{l+1} = 0$ for $l=L$. Estimates from all levels can then be
combined to give the final solution as:
\begin{equation}
  \hat{\ve{x}} = \sum_{l=0}^L \hat{\ve{y}}_{l}\:.
  \label{eq:multilevel_estimator}
\end{equation}
It is critical to note here that the term $(x^m_{l} - x^m_{l+1})$
in Eq~(\ref{eq:level_estimate}) consists of observations from the same
Markov chain computed over the space of the $l^{th}$ level. 

Although multigrid methods for linear algebra problems indicate that a
coarsening parameter of $M = 2$ is optimal such that every coarse
level is half the size of the previous finer level. However, Giles
reports that values of 4 and larger give optimal performance for these
particular estimators. In addition, as mentioned in
\S~\ref{sec:fourier_analysis}, because coarsening the problem makes
the modes appear more oscillatory, we must compute more samples at
coarser levels to balance the error amongst the levels. For this work
we will use the result reported by Heinrich for computing the number
of samples at each level:
\begin{equation}
  N_l = M^{-3(L-l)/2}N\:,
  \label{eq:level_histories}
\end{equation}
which provides a larger number of samples at the coarse levels. It
should be noted that Giles does not compute this quantity a priori but
instead first estimates the variance at each level and then computes
$N_l$ based on that variance.

Using the estimator given by Eq~(\ref{eq:multilevel_estimator}) we can
now define a multilevel Monte Carlo method for linear systems based on
the adjoint Monte Carlo solver given in \cite{evans_monte_2012}. To do
this, we must consider how to compute the expectation value
$E(\ve{x}_{l} - \ve{x}_{l+1})$ at each level in the problem. We are
required to construct observations on both grids from the same Markov
chain. Computing observations on the $l^{th}$ grid in this case is
simply the procedure for solving the adjoint Monte Carlo problem. We
could modify this procedure such that we tally observations on the
grid for the $l+1$ level through the use of a weight preserving linear
interpolation operator such that a tally in some state of level $l$ is
also a tally in some state of level $l+1$ with that state determined
by the operator. If this is the case, then we can actually avoid
tallying observations of the coarse grid altogether and simply apply
the linear interpolation operator the fine grid results once all
samples have been computed instead of modifying the Monte Carlo
sequence.

Using this idea we then define a \textit{prolongation operator},
$\ve{P}_l$, which maps a vector defined on grid $l+1$ to a
vector defined on grid $l$ and a \textit{restriction operator},
$\ve{R}_l$, which maps a vector defined on grid $l$ to a
vector defined on grid $l+1$. We can then define the following
equivalent expectation value for a given level $l$:
\begin{equation}
  E(\ve{x}_{l} - \ve{x}_{l+1}) = \Big(\ve{I} - \ve{P}_l
  \ve{R}_l\Big) \hat{\ve{x}}_{l}\:,
  \label{eq:level_expectation}
\end{equation}
with $\hat{\ve{x}}_{l}$ given by Eq~(\ref{eq:standard_estimator}) as
the solution on grid $l$. In this estimator, the action of the
restriction operator maps the tally to the coarse level to compute the
observations on that grid and the action of the prolongation operator
on the coarse tally maps the result back to the fine level. Using this
estimator, Algorithm~\ref{alg:mlamc} gives the multilevel Monte Carlo
algorithm for linear systems now with an arbitrary right hand side
vector, $\ve{b}$, and an initial solution guess, $\ve{x}^0$.
\begin{algorithm}[h!]
  \caption{Multilevel Monte Carlo Method}
  \label{alg:mlamc}
  \begin{algorithmic}[1]
    \For{ l = 0...L }
    \State $\ve{P}_l = P(\ve{A}_l)$
    \Comment{Build the prolongation and restriction operators for
      the $l^{th}$ level.}
    \State $\ve{R}_l = c \ve{P}_l^T$
    \State $\ve{r}_l = \ve{b}_l - \ve{A}_l \ve{x}_l^0$
    \Comment{Build the $l^{th}$ level residual.}
    \State $\ve{d}_l = \hat{\ve{A}}_l^{-1} \ve{r}_l$
    \Comment{Solve the $l^{th}$ level problem with adjoint Monte
      Carlo}
    \If{ l != L }
    \State $\ve{d}_l = (\ve{I} - \ve{P}_l\ve{R}_l) \ve{d}_{l}$
    \Comment{Apply the multilevel tally}
    \State $\ve{A}_{l+1} = \ve{R}_l \ve{A}_l \ve{P}_l$
    \Comment{Construct the next level.}
    \State $\ve{x}_{l+1}^0 = \ve{R}_l \ve{x}_l^0$
    \State $\ve{b}_{l+1} = \ve{R}_l \ve{b}_l$
    \EndIf
    \EndFor

    \For{ l = L...1 }
    \State $\ve{d}_{l-1} = ( \ve{I} + \ve{P}_{l} ) \ve{d}_{l}$
    \Comment{Collapse the tallies to the finest grid}
    \EndFor

    \State $\ve{x} = \ve{x}^0 + \ve{d}_0$

  \end{algorithmic}
\end{algorithm}
This algorithm permits each of the levels to be computed independently
and then combined appropriately with the prolongation and restriction
operators. Like other multigrid algorithms for linear systems, we are
required to produce a representation of the linear operator on each
level, $\ve{A}_l$, as well as the right hand side, $\ve{b}_l$. The
formulation of Algorithm~\ref{alg:mlamc} places no restrictions on the
form of $\ve{A}_l$, $\ve{b}_l$, $\ve{P}_l$, or $\ve{R}_l$
and therefore could be used with any multigrid scheme including those
arising from algebraic multigrid formulations. It should be noted,
however, that work to date has only tested the algorithm for the
Galerkin formulation presented in Algorithm~\ref{alg:mlamc}.

%%---------------------------------------------------------------------------%%
\section{Geometric Multigrid Results}
\label{sec:geometric_results}
To demonstrate the multilevel Monte Carlo algorithm we solve the
Poisson problem presented in \S~\ref{sec:fourier_analysis}, this time
with a grid of size $J = 1024$. For the coarsening parameter we will
first use a value of $M = 2$. For simplicity, we will assume a basic
geometric multigrid formulation of the linear problem per
\cite{briggs_multigrid} with the the following prolongation operator
for $M=2$:
\begin{subequations}
  \begin{gather}
    x^h_{2j} = x_j^{2h} \\
    x^h_{2j+1} = \frac{1}{2} ( x_j^{2h} + x_{j+1}^{2h} )\:,
  \end{gather}
  \label{eq:m2_prolongation}
\end{subequations}
for $0 \leq j \leq \frac{J}{2} - 1$ and the restriction operator:
\begin{equation}
  x^{2h}_j = \frac{1}{4} ( x_{2j-1}^{h} + 2 x_{2j}^{2h} + x_{2j+1}^{2h} )
  \label{eq:m2_restriction}
\end{equation}
for $1 \leq j \leq \frac{J}{2} - 1$.

To measure performance of the algorithm, we will introduce the
following standard figure of merit metric to assess the quality of the
Monte Carlo solver:
\begin{equation}
  FOM = \frac{1}{||\ve{e}||^2_{\infty} T}\:,
  \label{eq:figure_of_merit}
\end{equation}
where $||\ve{e}||_{\infty}$ is a measure of the variance of the
calculation and $T$ is the time required to solve the problem. The
larger the figure of merit, the more useful the solution scheme
is. For example, if two solutions schemes arrive at the same error but
the first calculation takes twice as long as the second, the figure of
merit will be twice as large for the second calculation.

We again perform calculations with $N=10,000$ and initial guesses of
wave numbers 1, 5 and 10 and vary the number of levels used in the
calculation. Tables~\ref{tab:k1_results}, \ref{tab:k5_results}, and
\ref{tab:k10_results} giving the results of these calculations. In
these tables, the RFOM column indicates the relative figure of merit
which has been normalized to the single level calculation. The results
indicate the performance of the multilevel method compared to the
original adjoint Monte Carlo method is excellent, offering a dramatic
improvement in time to solution and figure of merit values 10-100
times that of the single level solver.
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 0.022 & 119.1 & 1 \\
      2 & 13,535 & 0.026 & 72.0 & 1.14 \\
      3 & 14,785 & 0.035 & 33.2 & 1.38 \\
      4 & 15,226 & 0.039 & 13.7 & 2.63 \\
      5 & 15,382 & 0.027 & 5.5 & 13.86 \\
      6 & 15,437 & 0.036 & 2.3 & 19.21 \\
      7 & 15,456 & 0.045 & 0.98 & 28.00 \\
      8 & 15,462 & 0.081 & 0.41 & 20.66 \\
      9 & 15,464 & 0.267 & 0.17 & 11.72 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      1$ with $M = 2$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:k1_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 0.668 & 101.6 & 1 \\
      2 & 13,535 & 0.381 & 60.0 & 5.21 \\
      3 & 14,785 & 0.396 & 28.0 & 10.34 \\
      4 & 15,226 & 0.599 & 11.9 & 10.64 \\
      5 & 15,382 & 0.638 & 4.7 & 23.60 \\
      6 & 15,437 & 1.052 & 1.8 & 23.15 \\
      7 & 15,456 & 1.070 & 0.67 & 59.16 \\
      8 & 15,462 & 1.180 & 0.23 & 144.10 \\
      9 & 15,464 & 2.130 & 0.10 & 99.95 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      5$ with $M = 2$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:k5_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 1.81 & 98.6 & 1 \\
      2 & 13,535 & 1.67 & 59.6 & 1.94 \\
      3 & 14,785 & 2.41 & 27.1 & 2.05 \\
      4 & 15,226 & 3.20 & 11.5 & 2.74 \\
      5 & 15,382 & 1.85 & 4.93 & 19.14 \\
      6 & 15,437 & 3.89 & 1.95 & 10.95 \\
      7 & 15,456 & 3.79 & 0.78 & 28.68 \\
      8 & 15,462 & 7.08 & 0.30 & 21.62 \\
      9 & 15,464 & 11.7 & 0.11 & 21.45 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      10$ with $M = 2$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:k10_results}
\end{table}

Next we perform the calculations with $M=4$ using the following
prolongation operator:
\begin{subequations}
  \begin{gather}
    x^h_{4j} = x_j^{4h} \\
    x^h_{4j+1} = \frac{3}{4} x_j^{4h} + \frac{1}{4} x_{j+1}^{4h}\:\\
    x^h_{4j+2} = \frac{1}{2} x_j^{4h} + \frac{1}{2} x_{j+1}^{4h}\:\\
    x^h_{4j+3} = \frac{1}{4} x_j^{4h} + \frac{3}{4} x_{j+1}^{4h}\:,
  \end{gather}
  \label{eq:m4_prolongation}
\end{subequations}
for $0 \leq j \leq \frac{J}{4} - 1$ and the restriction operator:
\begin{equation}
  x^{4h}_j = \frac{1}{10} ( x_{4j-2}^{h} + 2 x_{4j-1}^{h} + 4
  x_{4j}^{4h} + 2 x_{4j+1}^{4h} + x_{4j+2}^{4h} )
  \label{eq:m4_restriction}
\end{equation}
for $1 \leq j \leq \frac{J}{4} - 1$. For these calculations we fix
$k=10$ and vary $N$ to study performance for the more difficult larger
wave numbers. Tables~\ref{tab:n4_results}, \ref{tab:n5_results}, and
\ref{tab:n6_results} give the results for $N=10,000$, $N=100,000$, and
$N=1,000,000$ respectively. Because of the time required to compute
problems with a small number of levels and large numbers of samples,
single level calculations were not performed for the last two
cases. For these cases, relative figure of merit values were computed
relative to the single level case with $N=10,000$.
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 1.81 & 98.6 & 1 \\
      2 & 11,250 & 2.64 & 18.2 & 2.53 \\
      3 & 11,406 & 2.61 & 3.1 & 15.15 \\
      4 & 11,425 & 4.56 & 0.47 & 33.04 \\
      5 & 11,427 & 16.5 & 0.07 & 16.95 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      10$ with $M = 4$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:n4_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      3 & 114,062 & 0.76 & 28.3 & 19.63 \\
      4 & 114,275 & 1.53 & 4.4 & 31.43 \\
      5 & 114,281 & 4.60 & 0.67 & 22.78 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      10$ with $M = 4$, $N = 100,000$ and run times reported in
      seconds.}}
  \label{tab:n5_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      4 & 1,142,758 & 0.56 & 41.8 & 25.00 \\
      5 & 1,142,822 & 1.42 & 6.49 & 24.68 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Geometric Multilevel Monte Carlo results for $k =
      10$ with $M = 4$, $N = 1,000,000$ and run times reported in
      seconds.}}
  \label{tab:n6_results}
\end{table}
Although Giles reports that $M \geq 4$ provides good performance in
the context of his calculations, the improvement is not as dramatic
for these cases. A coarser discretization of $M=4$ does provide a
small improvement in the time required to compute a single sample and
therefore enables slightly higher figures of merit. However, these
results do not rule out using $M=2$. This is an important result in
that both geometric and algebraic multigrid methods utilize $M=2$ and
therefore we may leverage the interpolation operators and other
infrastructure developed for those methods within the multilevel Monte
Carlo scheme presented here without a dramatic increase in
computational complexity.

%%---------------------------------------------------------------------------%%
\section{Algebraic Multigrid Results}
\label{sec:algebraic_results}
We next solve the same model problem, this time using an algebraic
multigrid formulation of the Galerkin problem. For these calculations
we used the ML multigrid library to construct $\ve{A}$, $\ve{P}$, and
$\ve{R}$ on each level with $\ve{M} \approx 3$
\cite{ml_manual}. Tables~\ref{tab:ml_k1_results},
\ref{tab:ml_k5_results}, and \ref{tab:ml_k10_results} give the results
of these calculations.
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 0.022 & 119.1 & 1 \\
      2 & 11,924 & 0.037 & 31.9 & 1.28 \\
      3 & 12,294 & 0.055 & 7.6 & 2.40 \\
      4 & 12,365 & 0.051 & 1.8 & 11.76 \\
      5 & 12,378 & 0.094 & 0.5 & 12.01 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Algebraic Multilevel Monte Carlo results for $k =
      1$ with $M = 3$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:ml_k1_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 0.668 & 101.6 & 1 \\
      2 & 11,924 & 0.834 & 30.8 & 2.12 \\
      3 & 12,294 & 1.140 & 7.5 & 4.63 \\
      4 & 12,365 & 0.975 & 1.6 & 29.09 \\
      5 & 12,378 & 2.240 & 0.4 & 25.10 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Algebraic Multilevel Monte Carlo results for $k =
      5$ with $M = 3$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:ml_k5_results}
\end{table}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{cllll}\hline\hline
      \multicolumn{1}{c}{\textbf{Levels}} & 
      \multicolumn{1}{l}{\textbf{Samples}} & 
      \multicolumn{1}{l}{\textbf{$||\ve{e}||_{\infty}$}} & 
      \multicolumn{1}{l}{\textbf{Time (s)}} & 
      \multicolumn{1}{l}{\textbf{RFOM}} \\
      \hline
      1 & 10,000 & 1.81 & 98.6 & 1 \\
      2 & 11,924 & 2.33 & 30.0 & 1.99 \\
      3 & 12,294 & 3.19 & 7.6 & 4.20 \\
      4 & 12,365 & 3.31 & 1.8 & 16.29 \\
      5 & 12,378 & 9.98 & 0.5 & 7.21 \\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{\textbf{Algebraic Multilevel Monte Carlo results for $k =
      10$ with $M = 3$, $N = 10,000$ and run times reported in
      seconds.}}
  \label{tab:ml_k10_results}
\end{table}
In general, the results show similar performance to the geometric
multigrid method, however, gains in figure of merit were not as
significant which may be in part due to the simplified selection of
sample size at each level. In addition, the Dirichlet conditions for
the model problem were explicitly included in the geometric
interpolation operators while the level operators generated by the
algebraic scheme did not. Most importantly, these results indicate
that Algorithm~\ref{alg:mlamc} can be used with general Galerkin
formulations of the multigrid problem.

%%---------------------------------------------------------------------------%%
\clearpage

\bibliographystyle{ieeetr}
\bibliography{references}

\closing
\caution
\end{document}
